{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luk1313/DUOC_ANALISIS_TICKETS/blob/main/ETL_JIRA_Pipeline_Robusto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffa4a54a",
      "metadata": {
        "id": "ffa4a54a"
      },
      "source": [
        "# ETL JIRA — Pipeline Robusto (Colab + Chile TZ + DuckDB + Parquet)\n",
        "\n",
        "Este notebook procesa masivamente JSON/JSONL/XLSX de JIRA, normaliza fechas a UTC (asumiendo Chile cuando falte tz), aplana a tablas (ticket, transition, worklog, comment), guarda en Parquet y consulta con DuckDB."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba919e20",
      "metadata": {
        "id": "ba919e20"
      },
      "source": [
        "## 1) Instalación de dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85776960",
      "metadata": {
        "id": "85776960"
      },
      "outputs": [],
      "source": [
        "!pip -q install duckdb pandas requests python-dateutil pytz python-dotenv tenacity sqlalchemy psycopg2-binary openpyxl fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install duckdb pandas requests python-dateutil pytz tenacity sqlalchemy psycopg2-binary matplotlib seaborn openpyxl supabase"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGzFAwqJWZMT",
        "outputId": "651d7f9b-3d7d-4d75-ba39-b6d5a105c30a"
      },
      "id": "LGzFAwqJWZMT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.12/dist-packages (1.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (2025.2)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.12/dist-packages (8.5.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.12/dist-packages (2.0.43)\n",
            "Requirement already satisfied: psycopg2-binary in /usr/local/lib/python3.12/dist-packages (2.9.11)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: supabase in /usr/local/lib/python3.12/dist-packages (2.22.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (4.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: realtime in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: supabase-functions in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: storage3 in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: supabase-auth in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: postgrest in /usr/local/lib/python3.12/dist-packages (from supabase) (2.22.0)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.12/dist-packages (from supabase) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
            "Requirement already satisfied: deprecation>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from postgrest->supabase) (2.1.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.9 in /usr/local/lib/python3.12/dist-packages (from postgrest->supabase) (2.11.10)\n",
            "Requirement already satisfied: yarl>=1.20.1 in /usr/local/lib/python3.12/dist-packages (from postgrest->supabase) (1.22.0)\n",
            "Requirement already satisfied: websockets<16,>=11 in /usr/local/lib/python3.12/dist-packages (from realtime->supabase) (15.0.1)\n",
            "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (2.10.1)\n",
            "Requirement already satisfied: strenum>=0.4.15 in /usr/local/lib/python3.12/dist-packages (from supabase-functions->supabase) (0.4.15)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<0.29,>=0.26->postgrest->supabase) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.4.2)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (43.0.3)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.12/dist-packages (from yarl>=1.20.1->postgrest->supabase) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl>=1.20.1->postgrest->supabase) (0.3.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (2.0.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest->supabase) (4.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae6abd3d",
      "metadata": {
        "id": "ae6abd3d"
      },
      "source": [
        "## 2) Configuración, carpetas y logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c709e125",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "c709e125",
        "outputId": "fb911841-faf7-4a94-92bc-71431fd70990"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**RAW:** `/content/jira/raw` — **PROCESSED:** `/content/jira/processed` — **DB:** `/content/jira/jira.duckdb`"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os, json, glob, re, unicodedata, logging, traceback\n",
        "from typing import Optional\n",
        "import pandas as pd, duckdb, pytz, requests\n",
        "from dateutil import parser\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from fuzzywuzzy import fuzz, process\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')\n",
        "\n",
        "# Ajusta estas rutas si lo deseas (en Colab puedes montar Drive y reemplazar BASE_DIR)\n",
        "BASE_DIR = \"/content/jira\"\n",
        "RAW_DIR  = f\"{BASE_DIR}/raw\"\n",
        "PROC_DIR = f\"{BASE_DIR}/processed\"\n",
        "DB_PATH  = f\"{BASE_DIR}/jira.duckdb\"\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "os.makedirs(PROC_DIR, exist_ok=True)\n",
        "\n",
        "BASE_URL = \"https://vps.clickbi.cl/rapi_ges/v1\"  # Gateway\n",
        "PROJECT_ID = \"10052\"\n",
        "DATE_FROM  = \"2024-01-01\"\n",
        "DATE_TO    = \"2024-12-31\"\n",
        "HEADERS    = {\"User-Agent\": \"jira-etl/1.0\"}\n",
        "# HEADERS[\"Authorization\"] = \"Basic TU_TOKEN\"  # <- si aplica\n",
        "\n",
        "CL_TZ_NAME = \"America/Santiago\"\n",
        "CL_TZ = pytz.timezone(CL_TZ_NAME)\n",
        "UTC = pytz.utc\n",
        "\n",
        "MODE = \"local\"  # 'upload' | 'local' | 'api'\n",
        "display(Markdown(f\"**RAW:** `{RAW_DIR}` — **PROCESSED:** `{PROC_DIR}` — **DB:** `{DB_PATH}`\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "246ab99e",
      "metadata": {
        "id": "246ab99e"
      },
      "source": [
        "## 3) Utilidades: fechas Chile⇄UTC y limpieza de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b1d70f",
      "metadata": {
        "id": "b2b1d70f"
      },
      "outputs": [],
      "source": [
        "def parse_ts_to_utc(ts: Optional[str]):\n",
        "    if not ts:\n",
        "        return None\n",
        "    dt = parser.isoparse(ts)\n",
        "    if dt.tzinfo is None:\n",
        "        import pytz\n",
        "        dt = pytz.timezone('America/Santiago').localize(dt)\n",
        "    return dt.astimezone(pytz.UTC).replace(tzinfo=None)\n",
        "\n",
        "def clean_text(x) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    s = unicodedata.normalize(\"NFC\", str(x))\n",
        "    s = re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "STATUS_MAP = {\"Open\":\"ABIERTO\",\"To Do\":\"ABIERTO\",\"In Progress\":\"EN_CURSO\",\"En Curso\":\"EN_CURSO\",\n",
        "              \"Blocked\":\"BLOQUEADO\",\"Done\":\"CERRADO\",\"Resolved\":\"RESUELTO\"}\n",
        "def norm_status(s):\n",
        "    if not s: return \"\"\n",
        "    return STATUS_MAP.get(s, str(s).upper())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f97b7774",
      "metadata": {
        "id": "f97b7774"
      },
      "source": [
        "## 4) Carga: Upload / Local / API Gateway"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434492ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "434492ae",
        "outputId": "e7016de3-7877-4dfa-c79a-86661ed80c07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "@retry(stop=stop_after_attempt(5), wait=wait_exponential(min=1, max=30))\n",
        "def api_list_keys(project_id: str, d1: str, d2: str) -> list:\n",
        "    url = f\"{BASE_URL}/lis_prj_jira_all/{project_id}/{d1}/{d2}\"\n",
        "    r = requests.get(url, headers=HEADERS, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    keys = []\n",
        "    for k in data:\n",
        "        if isinstance(k, str):\n",
        "            keys.append(k)\n",
        "        elif isinstance(k, dict):\n",
        "            keys.append(k.get('key') or k.get('issueKey') or k.get('id'))\n",
        "    return [k for k in keys if k]\n",
        "\n",
        "@retry(stop=stop_after_attempt(5), wait=wait_exponential(min=1, max=30))\n",
        "def api_get_issue_detail(issue_key: str) -> str:\n",
        "    url = f\"{BASE_URL}/get_jira_issue_detail/{issue_key}\"\n",
        "    r = requests.get(url, headers=HEADERS, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    payload = r.json()\n",
        "    path = os.path.join(RAW_DIR, f\"{issue_key}.json\")\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(payload, f, ensure_ascii=False)\n",
        "    return path\n",
        "\n",
        "def load_paths_local():\n",
        "    return sorted([os.path.join(RAW_DIR, p) for p in os.listdir(RAW_DIR)\n",
        "                   if p.lower().endswith(('.json','.jsonl','.xlsx'))])\n",
        "\n",
        "def decide_paths():\n",
        "    if MODE == 'local':\n",
        "        return load_paths_local()\n",
        "    elif MODE == 'api':\n",
        "        ks = api_list_keys(PROJECT_ID, DATE_FROM, DATE_TO)\n",
        "        return [api_get_issue_detail(k) for k in ks]\n",
        "    elif MODE == 'upload':\n",
        "        from google.colab import files\n",
        "        up = files.upload()\n",
        "        saved = []\n",
        "        for name, content in up.items():\n",
        "            path = os.path.join(RAW_DIR, name)\n",
        "            with open(path,'wb') as f: f.write(content)\n",
        "            saved.append(path)\n",
        "        return saved\n",
        "    else:\n",
        "        raise ValueError(\"MODE inválido\")\n",
        "\n",
        "FILES = decide_paths()\n",
        "len(FILES)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95448ea6",
      "metadata": {
        "id": "95448ea6"
      },
      "source": [
        "## 5) Aplanado (issue → 4 tablas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fccc569f",
      "metadata": {
        "id": "fccc569f"
      },
      "outputs": [],
      "source": [
        "def flatten_issue(j: dict, filename: str):\n",
        "    f = j.get('fields') or {}\n",
        "    ticket = {\n",
        "        'filename': filename,\n",
        "        'issue_key': j.get('key'),\n",
        "        'issue_id': j.get('id'),\n",
        "        'project_key': (f.get('project') or {}).get('key'),\n",
        "        'summary': clean_text(f.get('summary')),\n",
        "        'description': clean_text(f.get('description')),\n",
        "        'status': norm_status((f.get('status') or {}).get('name')),\n",
        "        'priority': (f.get('priority') or {}).get('name'),\n",
        "        'issuetype': (f.get('issuetype') or {}).get('name'),\n",
        "        'assignee': ((f.get('assignee') or {}) or {}).get('displayName'),\n",
        "        'created_utc': parse_ts_to_utc(f.get('created')),\n",
        "        'updated_utc': parse_ts_to_utc(f.get('updated')),\n",
        "        'resolutiondate_utc': parse_ts_to_utc(f.get('resolutiondate')),\n",
        "        'client': None, 'site_key': None, 'service_key': None,\n",
        "        'type': None\n",
        "    }\n",
        "    transitions, worklogs, comments = [], [], []\n",
        "    for h in (j.get('changelog') or {}).get('histories', []):\n",
        "        ts = parse_ts_to_utc(h.get('created'))\n",
        "        for it in h.get('items', []):\n",
        "            if it.get('field') == 'status':\n",
        "                transitions.append({\n",
        "                    'filename': filename,\n",
        "                    'issue_key': ticket['issue_key'],\n",
        "                    'changed_utc': ts,\n",
        "                    'from_status': norm_status(it.get('fromString')),\n",
        "                    'to_status': norm_status(it.get('toString')),\n",
        "                    'author': (h.get('author') or {}).get('displayName')\n",
        "                })\n",
        "    wl = f.get('worklog') or {}\n",
        "    for w in wl.get('worklogs', []) if isinstance(wl, dict) else []:\n",
        "        worklogs.append({\n",
        "            'filename': filename,\n",
        "            'issue_key': ticket['issue_key'],\n",
        "            'author': (w.get('author') or {}).get('displayName'),\n",
        "            'started_utc': parse_ts_to_utc(w.get('started')),\n",
        "            'time_spent_sec': w.get('timeSpentSeconds') or 0\n",
        "        })\n",
        "    cm = f.get('comment') or {}\n",
        "    for c in cm.get('comments', []) if isinstance(cm, dict) else []:\n",
        "        comments.append({\n",
        "            'filename': filename,\n",
        "            'issue_key': ticket['issue_key'],\n",
        "            'author': (c.get('author') or {}).get('displayName'),\n",
        "            'created_utc': parse_ts_to_utc(c.get('created')),\n",
        "            'body': clean_text(c.get('body'))\n",
        "        })\n",
        "    return ticket, transitions, worklogs, comments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aa46919",
      "metadata": {
        "id": "2aa46919"
      },
      "source": [
        "## 6) Procesamiento por lotes → Parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ba2b32d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "4ba2b32d",
        "outputId": "ac8c1634-ad4a-49c2-c034-10ab858813d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error en /content/jira/raw/CONSULTORTK_SERV-2055 (1) (1).json: cannot access local variable 'pytz' where it is not associated with a value\n",
            "ERROR:root:Error en /content/jira/raw/HISOTIRCO_JIRA_2024-12-31 (1).json: cannot access local variable 'pytz' where it is not associated with a value\n",
            "ERROR:root:Error en /content/jira/raw/JIRA_SERV-2055 (1) (1).json: cannot access local variable 'pytz' where it is not associated with a value\n",
            "ERROR:root:Error en /content/jira/raw/JIRA_TIK10006 (1).json: cannot access local variable 'pytz' where it is not associated with a value\n",
            "ERROR:root:Error en /content/jira/raw/SERV-2055 (1).json: cannot access local variable 'pytz' where it is not associated with a value\n",
            "ERROR:root:Error en /content/jira/raw/SERV-2055 (1).jsonl: cannot access local variable 'pytz' where it is not associated with a value\n",
            "/usr/local/lib/python3.12/dist-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "ERROR:root:Error en /content/jira/raw/SERV-2055_export (1).xlsx: No such keys(s): 'io.excel.zip.reader'\n",
            "ERROR:root:Error en /content/jira/raw/jira_10013 (1).json: cannot access local variable 'pytz' where it is not associated with a value\n",
            "ERROR:root:Error en /content/jira/raw/jira_10052 (1).json: cannot access local variable 'pytz' where it is not associated with a value\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "Index(['issue_key'], dtype='object')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4012837942.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Último lote guardado.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mprocess_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"### ✅ Parquet listos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4012837942.py\u001b[0m in \u001b[0;36mprocess_all\u001b[0;34m(paths, batch_size)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbt\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbtr\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbwl\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbcm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mdf_t\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'issue_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbt\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mdf_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mdf_wl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbwl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop_duplicates\u001b[0;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6816\u001b[0m         \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ignore_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6818\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6820\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mduplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   6948\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6949\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6950\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: Index(['issue_key'], dtype='object')"
          ]
        }
      ],
      "source": [
        "P_TICKET = f\"{PROC_DIR}/fact_ticket.parquet\"\n",
        "P_TRANS  = f\"{PROC_DIR}/fact_transition.parquet\"\n",
        "P_WL     = f\"{PROC_DIR}/fact_worklog.parquet\"\n",
        "P_COMM   = f\"{PROC_DIR}/fact_comment.parquet\"\n",
        "\n",
        "for p in [P_TICKET, P_TRANS, P_WL, P_COMM]:\n",
        "    if os.path.exists(p): os.remove(p)\n",
        "\n",
        "def append_parquet(df: pd.DataFrame, path: str):\n",
        "    if df is None or df.empty: return\n",
        "    if not os.path.exists(path):\n",
        "        df.to_parquet(path, index=False)\n",
        "    else:\n",
        "        con = duckdb.connect()\n",
        "        con.execute(\"CREATE TABLE t AS SELECT * FROM read_parquet(?)\", [path])\n",
        "        con.register(\"newdf\", df)\n",
        "        con.execute(\"INSERT INTO t SELECT * FROM newdf\")\n",
        "        con.execute(\"COPY t TO ? (FORMAT 'parquet', OVERWRITE 1)\", [path])\n",
        "        con.close()\n",
        "\n",
        "def process_all(paths: list, batch_size: int = 50):\n",
        "    bt, btr, bwl, bcm = [], [], [], []\n",
        "    done = 0\n",
        "    for fp in paths:\n",
        "        try:\n",
        "            if fp.lower().endswith('.xlsx'):\n",
        "                df = pd.read_excel(fp)\n",
        "                bt.extend(df.to_dict('records'))\n",
        "            elif fp.lower().endswith('.jsonl'):\n",
        "                with open(fp, 'r', encoding='utf-8') as f:\n",
        "                    for line in f:\n",
        "                        obj = json.loads(line)\n",
        "                        if isinstance(obj, dict) and 'fields' in obj:\n",
        "                            t,tr,wl,cm = flatten_issue(obj, os.path.basename(fp))\n",
        "                            bt.append(t); btr += tr; bwl += wl; bcm += cm\n",
        "            else:\n",
        "                with open(fp, 'r', encoding='utf-8') as f:\n",
        "                    obj = json.load(f)\n",
        "                if isinstance(obj, dict) and 'fields' in obj:\n",
        "                    t,tr,wl,cm = flatten_issue(obj, os.path.basename(fp))\n",
        "                    bt.append(t); btr += tr; bwl += wl; bcm += cm\n",
        "                elif isinstance(obj, dict) and 'issues' in obj:\n",
        "                    for issue in obj['issues']:\n",
        "                        t,tr,wl,cm = flatten_issue(issue, os.path.basename(fp))\n",
        "                        bt.append(t); btr += tr; bwl += wl; bcm += cm\n",
        "                elif isinstance(obj, list):\n",
        "                    for issue in obj:\n",
        "                        if isinstance(issue, dict) and 'fields' in issue:\n",
        "                            t,tr,wl,cm = flatten_issue(issue, os.path.basename(fp))\n",
        "                            bt.append(t); btr += tr; bwl += wl; bcm += cm\n",
        "            done += 1\n",
        "            if done % batch_size == 0:\n",
        "                df_t  = pd.DataFrame(bt).drop_duplicates(subset=['issue_key']) if bt else pd.DataFrame()\n",
        "                df_tr = pd.DataFrame(btr)\n",
        "                df_wl = pd.DataFrame(bwl)\n",
        "                df_cm = pd.DataFrame(bcm)\n",
        "                append_parquet(df_t, P_TICKET)\n",
        "                append_parquet(df_tr, P_TRANS)\n",
        "                append_parquet(df_wl, P_WL)\n",
        "                append_parquet(df_cm, P_COMM)\n",
        "                logging.info(f\"Lote guardado: {done}/{len(paths)}\")\n",
        "                bt,btr,bwl,bcm = [],[],[],[]\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error en {fp}: {e}\")\n",
        "\n",
        "    if bt or btr or bwl or bcm:\n",
        "        df_t  = pd.DataFrame(bt).drop_duplicates(subset=['issue_key']) if bt else pd.DataFrame()\n",
        "        df_tr = pd.DataFrame(btr)\n",
        "        df_wl = pd.DataFrame(bwl)\n",
        "        df_cm = pd.DataFrame(bcm)\n",
        "        append_parquet(df_t, P_TICKET)\n",
        "        append_parquet(df_tr, P_TRANS)\n",
        "        append_parquet(df_wl, P_WL)\n",
        "        append_parquet(df_cm, P_COMM)\n",
        "        logging.info(\"Último lote guardado.\")\n",
        "\n",
        "process_all(FILES, batch_size=40)\n",
        "display(Markdown(\"### ✅ Parquet listos\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d6b5200",
      "metadata": {
        "id": "4d6b5200"
      },
      "source": [
        "## 7) Consultas con DuckDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64a620a1",
      "metadata": {
        "id": "64a620a1"
      },
      "outputs": [],
      "source": [
        "con = duckdb.connect(DB_PATH)\n",
        "con.execute(f\"CREATE OR REPLACE VIEW fact_ticket     AS SELECT * FROM read_parquet('{P_TICKET}');\")\n",
        "con.execute(f\"CREATE OR REPLACE VIEW fact_transition AS SELECT * FROM read_parquet('{P_TRANS}');\")\n",
        "con.execute(f\"CREATE OR REPLACE VIEW fact_worklog    AS SELECT * FROM read_parquet('{P_WL}');\")\n",
        "con.execute(f\"CREATE OR REPLACE VIEW fact_comment    AS SELECT * FROM read_parquet('{P_COMM}');\")\n",
        "\n",
        "print('Tickets:', con.execute('SELECT COUNT(*) FROM fact_ticket').fetchone()[0])\n",
        "con.execute('SELECT status, COUNT(*) n FROM fact_ticket GROUP BY 1 ORDER BY n DESC').df()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e3dcfe",
      "metadata": {
        "id": "44e3dcfe"
      },
      "source": [
        "## 8) Exportación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7808eea3",
      "metadata": {
        "id": "7808eea3"
      },
      "outputs": [],
      "source": [
        "con.execute(f\"COPY (SELECT * FROM fact_ticket)     TO '{PROC_DIR}/fact_ticket.parquet' (FORMAT 'parquet', OVERWRITE 1)\")\n",
        "con.execute(f\"COPY (SELECT * FROM fact_transition) TO '{PROC_DIR}/fact_transition.parquet' (FORMAT 'parquet', OVERWRITE 1)\")\n",
        "con.execute(f\"COPY (SELECT * FROM fact_comment)    TO '{PROC_DIR}/fact_comment.parquet' (FORMAT 'parquet', OVERWRITE 1)\")\n",
        "con.execute(f\"COPY (SELECT * FROM fact_worklog)    TO '{PROC_DIR}/fact_worklog.parquet' (FORMAT 'parquet', OVERWRITE 1)\")\n",
        "\n",
        "df_estado = con.execute('SELECT status, COUNT(*) n FROM fact_ticket GROUP BY 1 ORDER BY n DESC').df()\n",
        "with pd.ExcelWriter(f\"{PROC_DIR}/jira_resumen.xlsx\") as xw:\n",
        "    df_estado.to_excel(xw, sheet_name='tickets_por_estado', index=False)\n",
        "print('Exportación lista en processed/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# # ETL JIRA: Análisis Completo por Cliente/Archivo (Colab + Chile TZ + Supabase + Gráficos)\n",
        "#\n",
        "# Este notebook procesa archivos JSON/Excel de JIRA uno por uno, identificando clientes/instalaciones, creando DataFrames separados, integrando con Supabase (o DuckDB), y generando visualizaciones con explicaciones en Markdown. Diseñado para ser reutilizable y escalable.\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Instalación y Configuración\n",
        "# Instalamos dependencias, definimos constantes y configuramos el entorno. Ingresa tus credenciales de Supabase si planeas usar esa opción."
      ],
      "metadata": {
        "id": "DZRzOfJCWy0T"
      },
      "id": "DZRzOfJCWy0T"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import pandas as pd\n",
        "import duckdb\n",
        "import requests\n",
        "from dateutil.parser import parse\n",
        "import pytz\n",
        "import unicodedata\n",
        "import re\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sqlalchemy import create_engine\n",
        "from IPython.display import Markdown, display\n",
        "import uuid"
      ],
      "metadata": {
        "id": "kXceABy8WvwF"
      },
      "id": "kXceABy8WvwF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración de logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Constantes\n",
        "BASE_URL = \"https://vps.clickbi.cl/rapi_ges/v1\"\n",
        "PROJECT_ID = \"10052\"\n",
        "DATE_FROM = \"2024-01-01\"\n",
        "DATE_TO = \"2024-12-31\"\n",
        "HEADERS = {\"User-Agent\": \"jira-etl/1.0\"}\n",
        "CL_TZ = \"America/Santiago\"\n",
        "RAW_DIR = \"/content/data/raw\"\n",
        "PROCESSED_DIR = \"/content/data/processed\"\n",
        "DB_PATH = \"/content/jira.duckdb\"\n"
      ],
      "metadata": {
        "id": "-1XdIy8mWuvG"
      },
      "id": "-1XdIy8mWuvG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración Supabase (modifica con tus datos)\n",
        "USE_SUPABASE = False  # Cambia a True si usas Supabase\n",
        "SUPABASE_URL = \"postgresql://postgres:[YOUR-PASSWORD]@db.[project].supabase.co:5432/postgres\"  # Reemplaza\n",
        "SUPABASE_ANON_KEY = \"[YOUR-ANON-KEY]\"  # Reemplaza\n",
        "SELECTED_FILE = None  # Deja None para procesar todos, o especifica 'SERV-2055.json'"
      ],
      "metadata": {
        "id": "s6gfO8AAW5_n"
      },
      "id": "s6gfO8AAW5_n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear directorios\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "os.makedirs(PROCESSED_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "skYwobDmW8O9"
      },
      "id": "skYwobDmW8O9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## 2. Carga de Datos\n",
        "# Sube archivos desde tu computador o usa los existentes en /content/data/raw/. Detecta clientes a partir del nombre del archivo o JSON."
      ],
      "metadata": {
        "id": "M9wb8SLzXFYk"
      },
      "id": "M9wb8SLzXFYk"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWa594ozXE6C"
      },
      "id": "vWa594ozXE6C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "def load_uploaded_files():\n",
        "    \"\"\"Carga archivos JSON o Excel desde el computador.\"\"\"\n",
        "    logging.info(\"Subiendo archivos desde el computador...\")\n",
        "    uploaded = files.upload()\n",
        "    for filename, content in uploaded.items():\n",
        "        filepath = os.path.join(RAW_DIR, filename)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            f.write(content)\n",
        "    logging.info(f\"Archivos subidos: {list(uploaded.keys())}\")\n",
        "    return list(uploaded.keys())\n",
        "\n",
        "def extract_client_from_file(file_name, json_data=None):\n",
        "    \"\"\"Extrae el nombre del cliente del archivo o JSON.\"\"\"\n",
        "    # Intenta extraer de file_name (e.g., SERV-2055 → SERV)\n",
        "    client_match = re.match(r'(.+?)-(\\d+)', file_name)\n",
        "    client = client_match.group(1) if client_match else file_name.split('.')[0]\n",
        "    # Si hay JSON, busca en summary (e.g., \"cliente: XYZ\")\n",
        "    if json_data and 'fields' in json_data:\n",
        "        summary = json_data.get('fields', {}).get('summary', '')\n",
        "        client_match_json = re.search(r'cliente:\\s*(\\w+)', summary, re.IGNORECASE)\n",
        "        client = client_match_json.group(1) if client_match_json else client\n",
        "    return client.strip()\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Carga datos según disponibilidad.\"\"\"\n",
        "    files_loaded = load_uploaded_files() if not os.listdir(RAW_DIR) else [f for f in os.listdir(RAW_DIR) if f.endswith(('.json', '.xlsx'))]\n",
        "    files_to_process = [SELECTED_FILE] if SELECTED_FILE and SELECTED_FILE in files_loaded else files_loaded\n",
        "    file_client_map = {}\n",
        "    for f in files_to_process:\n",
        "        filepath = os.path.join(RAW_DIR, f)\n",
        "        client = None\n",
        "        if f.endswith('.json'):\n",
        "            with open(filepath, 'r') as file:\n",
        "                data = json.load(file)\n",
        "                client = extract_client_from_file(f, data)\n",
        "        else:\n",
        "            client = extract_client_from_file(f)\n",
        "        file_client_map[f] = client\n",
        "    logging.info(f\"Archivos a procesar: {files_to_process}\")\n",
        "    df_summary = pd.DataFrame(list(file_client_map.items()), columns=['Archivo', 'Cliente_Detectado'])\n",
        "    display(df_summary)\n",
        "    return files_to_process, file_client_map"
      ],
      "metadata": {
        "id": "fuIxMK1gXB39"
      },
      "id": "fuIxMK1gXB39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datos\n",
        "files_loaded, file_client_map = load_data()"
      ],
      "metadata": {
        "id": "7MxQlwPhXLn7"
      },
      "id": "7MxQlwPhXLn7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## 3. Procesamiento Granular (ETL por Archivo/Cliente)\n",
        "# Procesa cada archivo, asigna cliente, y crea DataFrames separados."
      ],
      "metadata": {
        "id": "KataMJtsXzCl"
      },
      "id": "KataMJtsXzCl"
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import re\n",
        "import unicodedata\n",
        "from dateutil.parser import parse\n",
        "import pytz\n",
        "\n",
        "# Configuración de logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Constantes (definidas como cadenas para evitar problemas con timezone)\n",
        "CL_TZ_STR = \"America/Santiago\"\n",
        "\n",
        "def parse_ts_to_utc(timestamp):\n",
        "    \"\"\"\n",
        "    Convierte un timestamp a UTC, asumiendo que si no tiene zona horaria, está en America/Santiago.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dt = parse(timestamp)\n",
        "        if dt.tzinfo is None:\n",
        "            cl_tz = pytz.timezone(CL_TZ_STR)\n",
        "            dt = cl_tz.localize(dt)\n",
        "        return dt.astimezone(pytz.UTC)\n",
        "    except ValueError as ve:\n",
        "        logging.warning(f\"Error parseando {timestamp} como fecha: {ve}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error inesperado parseando {timestamp}: {e}\")\n",
        "        return None\n",
        "\n",
        "def utc_to_chile(dt_utc):\n",
        "    \"\"\"\n",
        "    Convierte un datetime en UTC a America/Santiago, asegurando que sea timezone-aware.\n",
        "    \"\"\"\n",
        "    if dt_utc is None:\n",
        "        return None\n",
        "    if dt_utc.tzinfo is None:\n",
        "        dt_utc = pytz.UTC.localize(dt_utc)\n",
        "    return dt_utc.astimezone(pytz.timezone(CL_TZ_STR))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Normaliza y limpia texto eliminando caracteres no deseados.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    text = re.sub(r'[^\\w\\s-]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "def flatten_issue_per_client(issue, file_name, client):\n",
        "    \"\"\"\n",
        "    Aplana un issue JIRA individual en listas de tickets, transiciones, comentarios y worklogs.\n",
        "    \"\"\"\n",
        "    tickets, transitions, comments, worklogs = [], [], [], []\n",
        "    try:\n",
        "        if not isinstance(issue, dict) or 'fields' not in issue:\n",
        "            logging.warning(f\"Estructura de issue inválida en {file_name}\")\n",
        "            return tickets, transitions, comments, worklogs\n",
        "\n",
        "        created_utc = parse_ts_to_utc(issue.get('fields', {}).get('created', ''))\n",
        "        ticket = {\n",
        "            'issue_key': issue.get('key', ''),\n",
        "            'summary': clean_text(issue.get('fields', {}).get('summary', '')),\n",
        "            'status': issue.get('fields', {}).get('status', {}).get('name', ''),\n",
        "            'assignee': issue.get('fields', {}).get('assignee', {}).get('displayName', ''),\n",
        "            'created_utc': created_utc,\n",
        "            'created_cl': utc_to_chile(created_utc),\n",
        "            'resolutiondate_utc': parse_ts_to_utc(issue.get('fields', {}).get('resolutiondate', '')),\n",
        "            'cliente': client,\n",
        "            'archivo_origen': file_name\n",
        "        }\n",
        "\n",
        "        if ticket['issue_key']:\n",
        "            # Normalizar estado\n",
        "            status_map = {'Open': 'ABIERTO', 'In Progress': 'EN_CURSO', 'Closed': 'RESUELTO'}\n",
        "            ticket['status'] = status_map.get(ticket['status'], ticket['status'])\n",
        "            tickets.append(ticket)\n",
        "\n",
        "            # Transiciones\n",
        "            for hist in issue.get('changelog', {}).get('histories', []):\n",
        "                for item in hist.get('items', []):\n",
        "                    if item.get('field') == 'status':\n",
        "                        transitions.append({\n",
        "                            'issue_key': ticket['issue_key'],\n",
        "                            'from_status': item.get('fromString', ''),\n",
        "                            'to_status': item.get('toString', ''),\n",
        "                            'transition_ts_utc': parse_ts_to_utc(hist.get('created', '')),\n",
        "                            'author': hist.get('author', {}).get('displayName', ''),\n",
        "                            'cliente': client,\n",
        "                            'archivo_origen': file_name\n",
        "                        })\n",
        "\n",
        "            # Comentarios\n",
        "            for comment in issue.get('fields', {}).get('comment', {}).get('comments', []):\n",
        "                comments.append({\n",
        "                    'issue_key': ticket['issue_key'],\n",
        "                    'author': comment.get('author', {}).get('displayName', ''),\n",
        "                    'created_utc': parse_ts_to_utc(comment.get('created', '')),\n",
        "                    'body_clean': clean_text(comment.get('body', '')),\n",
        "                    'cliente': client,\n",
        "                    'archivo_origen': file_name\n",
        "                })\n",
        "\n",
        "            # Worklogs\n",
        "            for worklog in issue.get('fields', {}).get('worklog', {}).get('worklogs', []):\n",
        "                worklogs.append({\n",
        "                    'issue_key': ticket['issue_key'],\n",
        "                    'author': worklog.get('author', {}).get('displayName', ''),\n",
        "                    'time_spent_sec': worklog.get('timeSpentSeconds', 0),\n",
        "                    'started_utc': parse_ts_to_utc(worklog.get('started', '')),\n",
        "                    'cliente': client,\n",
        "                    'archivo_origen': file_name\n",
        "                })\n",
        "        else:\n",
        "            logging.warning(f\"Ignorando issue sin clave en {file_name}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error procesando issue en {file_name}: {e}\")\n",
        "    return tickets, transitions, comments, worklogs\n",
        "\n",
        "def process_files(files, client_map):\n",
        "    \"\"\"\n",
        "    Procesa archivos JSON/Excel y crea DataFrames consolidados por cliente.\n",
        "    \"\"\"\n",
        "    client_data = {}\n",
        "    for f in files:\n",
        "        filepath = os.path.join(RAW_DIR, f)\n",
        "        client = client_map[f]\n",
        "        all_tickets, all_transitions, all_comments, all_worklogs = [], [], [], []\n",
        "        try:\n",
        "            if f.lower().endswith('.json'):\n",
        "                with open(filepath, 'r') as file:\n",
        "                    data = json.load(file)\n",
        "                if isinstance(data, dict):\n",
        "                    if 'issues' in data:\n",
        "                        issues = data['issues']\n",
        "                    elif 'fields' in data:\n",
        "                        issues = [data]\n",
        "                    else:\n",
        "                        issues = []\n",
        "                elif isinstance(data, list):\n",
        "                    issues = data\n",
        "                else:\n",
        "                    logging.warning(f\"Estructura JSON no soportada en {f}, saltando archivo\")\n",
        "                    continue\n",
        "\n",
        "                for issue in issues:\n",
        "                    t, tr, c, wl = flatten_issue_per_client(issue, f, client)\n",
        "                    all_tickets.extend(t)\n",
        "                    all_transitions.extend(tr)\n",
        "                    all_comments.extend(c)\n",
        "                    all_worklogs.extend(wl)\n",
        "\n",
        "            elif f.lower().endswith('.xlsx'):\n",
        "                df = pd.read_excel(filepath, engine='openpyxl')\n",
        "                all_tickets = df.assign(cliente=client, archivo_origen=f).to_dict('records')\n",
        "                all_transitions, all_comments, all_worklogs = [], [], []\n",
        "\n",
        "            else:\n",
        "                logging.warning(f\"Tipo de archivo no soportado: {f}, saltando\")\n",
        "                continue\n",
        "\n",
        "            if not all_tickets:\n",
        "                logging.warning(f\"No se encontraron tickets válidos en {f}\")\n",
        "                continue\n",
        "\n",
        "            # Crear DataFrames\n",
        "            df_tickets = pd.DataFrame(all_tickets).drop_duplicates(subset=['issue_key', 'cliente'])\n",
        "            df_transitions = pd.DataFrame(all_transitions).drop_duplicates()\n",
        "            df_comments = pd.DataFrame(all_comments).drop_duplicates()\n",
        "            df_worklogs = pd.DataFrame(all_worklogs).drop_duplicates()\n",
        "\n",
        "            # Consolidar datos por cliente\n",
        "            if client not in client_data:\n",
        "                client_data[client] = {\n",
        "                    'tickets': df_tickets,\n",
        "                    'transitions': df_transitions,\n",
        "                    'comments': df_comments,\n",
        "                    'worklogs': df_worklogs\n",
        "                }\n",
        "            else:\n",
        "                client_data[client]['tickets'] = pd.concat([client_data[client]['tickets'], df_tickets]).drop_duplicates(subset=['issue_key', 'cliente'])\n",
        "                client_data[client]['transitions'] = pd.concat([client_data[client]['transitions'], df_transitions]).drop_duplicates()\n",
        "                client_data[client]['comments'] = pd.concat([client_data[client]['comments'], df_comments]).drop_duplicates()\n",
        "                client_data[client]['worklogs'] = pd.concat([client_data[client]['worklogs'], df_worklogs]).drop_duplicates()\n",
        "\n",
        "            # Guardar datos por archivo (opcional)\n",
        "            base_name = os.path.splitext(f)[0]\n",
        "            if not df_tickets.empty:\n",
        "                df_tickets.to_parquet(os.path.join(PROCESSED_DIR, f\"{client}_{base_name}_tickets.parquet\"))\n",
        "            if not df_transitions.empty:\n",
        "                df_transitions.to_parquet(os.path.join(PROCESSED_DIR, f\"{client}_{base_name}_transitions.parquet\"))\n",
        "            if not df_comments.empty:\n",
        "                df_comments.to_parquet(os.path.join(PROCESSED_DIR, f\"{client}_{base_name}_comments.parquet\"))\n",
        "            if not df_worklogs.empty:\n",
        "                df_worklogs.to_parquet(os.path.join(PROCESSED_DIR, f\"{client}_{base_name}_worklogs.parquet\"))\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error procesando archivo {f}: {e}\")\n",
        "\n",
        "    # Guardar datos consolidados por cliente\n",
        "    for client, data in client_data.items():\n",
        "        if not data['tickets'].empty:\n",
        "            data['tickets'].to_parquet(os.path.join(PROCESSED_DIR, f\"{client}_tickets.parquet\"))\n",
        "        if not data['transitions'].empty:\n",
        "            data['transitions'].to_parquet(os.path.join(PROCESSED_DIR, f\"{client}_transitions.parquet\"))\n",
        "        if not data['comments'].empty:\n",
        "            data['comments'].to_parquet(os.path.join(PROCESSED_DIR, f\"{client}_comments.parquet\"))\n",
        "        if not data['worklogs'].empty:\n",
        "            data['worklogs'].to_parquet(os.path.join(PROCESSED_DIR, f\"{client}_worklogs.parquet\"))\n",
        "\n",
        "    return client_data\n"
      ],
      "metadata": {
        "id": "NpBmtI7WY06k"
      },
      "id": "NpBmtI7WY06k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Procesar archivos\n",
        "# Define PROCESSED_DIR if it's not already defined\n",
        "if 'PROCESSED_DIR' not in locals():\n",
        "    PROCESSED_DIR = \"/content/data/processed\"\n",
        "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
        "\n",
        "client_data = process_files(files_loaded, file_client_map)"
      ],
      "metadata": {
        "id": "rpJhZN55X-bj"
      },
      "id": "rpJhZN55X-bj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## 4. DataFrames por Cliente y Visualización Inicial\n",
        "# Muestra DataFrames y genera resúmenes dinámicos."
      ],
      "metadata": {
        "id": "XmeFGpBNZ-_0"
      },
      "id": "XmeFGpBNZ-_0"
    },
    {
      "cell_type": "code",
      "source": [
        "for client, data in client_data.items():\n",
        "    md = f\"# Análisis para Cliente: {client}\\n- Registros (Tickets): {data['tickets'].shape[0]}\\n- Estados únicos: {data['tickets']['status'].unique()}\"\n",
        "    display(Markdown(md))\n",
        "    display(data['tickets'].head(3))\n",
        "    print(f\"Descripción estadística:\\n{data['tickets'].describe()}\")"
      ],
      "metadata": {
        "id": "WCY2HjOUaCFs"
      },
      "id": "WCY2HjOUaCFs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## 5. Integración con Supabase (o DuckDB Fallback)\n",
        "# Conecta y carga datos a Supabase o DuckDB."
      ],
      "metadata": {
        "id": "sxLNp0DHarRF"
      },
      "id": "sxLNp0DHarRF"
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_SUPABASE:\n",
        "    engine = create_engine(SUPABASE_URL)\n",
        "    conn = engine.connect()\n",
        "    # Crear tablas en Supabase\n",
        "    conn.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS fact_ticket (\n",
        "            issue_key VARCHAR,\n",
        "            summary VARCHAR,\n",
        "            status VARCHAR,\n",
        "            assignee VARCHAR,\n",
        "            created_utc TIMESTAMP,\n",
        "            created_cl TIMESTAMP,\n",
        "            resolutiondate_utc TIMESTAMP,\n",
        "            cliente VARCHAR,\n",
        "            archivo_origen VARCHAR,\n",
        "            PRIMARY KEY (issue_key, cliente)\n",
        "        );\n",
        "    \"\"\")\n",
        "    conn.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS fact_transition (\n",
        "            issue_key VARCHAR,\n",
        "            from_status VARCHAR,\n",
        "            to_status VARCHAR,\n",
        "            transition_ts_utc TIMESTAMP,\n",
        "            author VARCHAR,\n",
        "            cliente VARCHAR,\n",
        "            archivo_origen VARCHAR\n",
        "        );\n",
        "    \"\"\")\n",
        "    # Similar para fact_comment, fact_worklog\n",
        "    for client, data in client_data.items():\n",
        "        for df_name, df in data.items():\n",
        "            df.to_sql(f'fact_{df_name.split(\"_\")[0]}', engine, if_exists='append', index=False)\n",
        "else:\n",
        "    conn = duckdb.connect(DB_PATH)\n",
        "    conn.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS fact_ticket (\n",
        "            issue_key VARCHAR,\n",
        "            summary VARCHAR,\n",
        "            status VARCHAR,\n",
        "            assignee VARCHAR,\n",
        "            created_utc TIMESTAMP,\n",
        "            created_cl TIMESTAMP,\n",
        "            resolutiondate_utc TIMESTAMP,\n",
        "            cliente VARCHAR,\n",
        "            archivo_origen VARCHAR\n",
        "        );\n",
        "    \"\"\")\n",
        "    conn.register('df_tickets', pd.concat([d['tickets'] for d in client_data.values()]))\n",
        "    conn.execute(\"INSERT OR REPLACE INTO fact_ticket SELECT * FROM df_tickets\")"
      ],
      "metadata": {
        "id": "KswxHYl6avnK"
      },
      "id": "KswxHYl6avnK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validación\n",
        "if USE_SUPABASE:\n",
        "    result = conn.execute(\"SELECT cliente, COUNT(*) FROM fact_ticket GROUP BY cliente\").fetchall()\n",
        "else:\n",
        "    result = conn.query(\"SELECT cliente, COUNT(*) FROM fact_ticket GROUP BY cliente\").to_df()\n",
        "display(pd.DataFrame(result, columns=['Cliente', 'Conteo']))\n"
      ],
      "metadata": {
        "id": "hbmUZDPPa2qU"
      },
      "id": "hbmUZDPPa2qU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## 6. Consultas Analíticas por Cliente\n",
        "# Ejecuta queries específicas por cliente."
      ],
      "metadata": {
        "id": "dbHHanY4bCP0"
      },
      "id": "dbHHanY4bCP0"
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "for client, data in client_data.items():\n",
        "    if USE_SUPABASE:\n",
        "        query = f\"SELECT status, COUNT(*) as count FROM fact_ticket WHERE cliente='{client}' GROUP BY status\"\n",
        "        df_status = pd.read_sql_query(query, engine)\n",
        "        query_top = f\"SELECT issue_key, COUNT(*) as incidencias FROM fact_transition WHERE cliente='{client}' GROUP BY issue_key ORDER BY incidencias DESC LIMIT 5\"\n",
        "        df_top = pd.read_sql_query(query_top, engine)\n",
        "    else:\n",
        "        df_status = conn.query(f\"SELECT status, COUNT(*) as count FROM fact_ticket WHERE cliente='{client}' GROUP BY status\").to_df()\n",
        "        df_top = conn.query(f\"SELECT issue_key, COUNT(*) as incidencias FROM fact_transition WHERE cliente='{client}' GROUP BY issue_key ORDER BY incidencias DESC LIMIT 5\").to_df()\n",
        "\n",
        "    top_ticket = df_top.iloc[0]['issue_key'] if not df_top.empty else \"N/A\"\n",
        "    md = f\"### Análisis para {client}\\n- Distribución de estados: {df_status.to_dict()}\\n- Ticket con más incidencias: {top_ticket} ({df_top.iloc[0]['incidencias']} transiciones)\"\n",
        "    display(Markdown(md))\n",
        "    display(df_status)"
      ],
      "metadata": {
        "id": "GiDfzWxjbEfv"
      },
      "id": "GiDfzWxjbEfv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## 7. Visualizaciones por Cliente/JSON\n",
        "# Genera gráficos con explicaciones."
      ],
      "metadata": {
        "id": "QNi0mXmAbQqV"
      },
      "id": "QNi0mXmAbQqV"
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 7. Visualizaciones por Cliente/JSON\n",
        "# Genera gráficos con explicaciones.\n",
        "\n",
        "# %% [code]\n",
        "for client, data in client_data.items():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(data=data['tickets'], x='status', hue='status')\n",
        "    plt.title(f'Distribución de Tickets por Estado - {client}')\n",
        "    plt.xlabel('Estado')\n",
        "    plt.ylabel('Conteo')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.savefig(os.path.join(PROCESSED_DIR, f'{client}_status_plot.png'))\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    data['transitions'].groupby('issue_key').size().sort_values(ascending=False).head(10).plot.bar()\n",
        "    plt.title(f'Top 10 Tickets por Incidencias/Mantenimientos - {client}')\n",
        "    plt.xlabel('Issue Key')\n",
        "    plt.ylabel('Conteo de Transiciones')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.savefig(os.path.join(PROCESSED_DIR, f'{client}_top_tickets_plot.png'))\n",
        "    plt.show()\n",
        "\n",
        "    md = f\"### Visualización para {client}\\n- **Insight**: El ticket con más transiciones es {data['transitions'].groupby('issue_key').size().idxmax()} con {data['transitions'].groupby('issue_key').size().max()} movimientos.\\n- La mayoría de los tickets están en estado {data['tickets']['status'].mode().iloc[0]}.\"\n",
        "    display(Markdown(md))"
      ],
      "metadata": {
        "id": "YgRVggQGbJbP"
      },
      "id": "YgRVggQGbJbP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## 8. Exportación por Cliente y Unificada\n",
        "# Exporta DataFrames a Excel y Parquet."
      ],
      "metadata": {
        "id": "qgjqhzjIbXYE"
      },
      "id": "qgjqhzjIbXYE"
    },
    {
      "cell_type": "code",
      "source": [
        "for client, data in client_data.items():\n",
        "    with pd.ExcelWriter(os.path.join(PROCESSED_DIR, f'{client}_export.xlsx')) as writer:\n",
        "        data['tickets'].to_excel(writer, sheet_name='Tickets', index=False)\n",
        "        data['transitions'].to_excel(writer, sheet_name='Transitions', index=False)\n",
        "        data['comments'].to_excel(writer, sheet_name='Comments', index=False)\n",
        "        data['worklogs'].to_excel(writer, sheet_name='Worklogs', index=False)\n",
        "\n",
        "# Exportación unificada\n",
        "all_tickets = pd.concat([d['tickets'] for d in client_data.values()])\n",
        "all_tickets.to_parquet(os.path.join(PROCESSED_DIR, 'all_tickets.parquet'))\n",
        "with pd.ExcelWriter(os.path.join(PROCESSED_DIR, 'jira_export_all.xlsx')) as writer:\n",
        "    all_tickets.to_excel(writer, sheet_name='All_Tickets', index=False)"
      ],
      "metadata": {
        "id": "AIPlFSVTbagn"
      },
      "id": "AIPlFSVTbagn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## 9. Análisis Multi-Cliente (Opcional)\n",
        "# Une todos los datos y genera análisis cruzado."
      ],
      "metadata": {
        "id": "jxXABw-ibdjG"
      },
      "id": "jxXABw-ibdjG"
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.concat([d['tickets'] for d in client_data.values()])\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(pd.crosstab(df_all['cliente'], df_all['status']), annot=True, fmt='d')\n",
        "plt.title('Conteo de Tickets por Cliente y Estado')\n",
        "plt.savefig(os.path.join(PROCESSED_DIR, 'heatmap_client_status.png'))\n",
        "plt.show()\n",
        "\n",
        "md = f\"### Análisis Multi-Cliente\\n- Total de tickets: {len(df_all)}\\n- Cliente con más tickets: {df_all['cliente'].value_counts().idxmax()} con {df_all['cliente'].value_counts().max()} registros.\"\n",
        "display(Markdown(md))"
      ],
      "metadata": {
        "id": "vqIIgX_MbgDT"
      },
      "id": "vqIIgX_MbgDT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ## 10. Documentación Dinámica Final\n",
        "# Resumen global de los análisis."
      ],
      "metadata": {
        "id": "RUdthR2zbuAS"
      },
      "id": "RUdthR2zbuAS"
    },
    {
      "cell_type": "code",
      "source": [
        "global_summary = f\"\"\"\n",
        "# Resumen Final\n",
        "- **Total Clientes Procesados**: {len(client_data)}\n",
        "- **Total Tickets**: {len(all_tickets)}\n",
        "- **Porcentaje de Nulos**: {all_tickets.isnull().mean().to_dict()}\n",
        "- **Top Insight**: El cliente {all_tickets['cliente'].value_counts().idxmax()} tiene el mayor número de tickets ({all_tickets['cliente'].value_counts().max()}).\n",
        "\"\"\"\n",
        "display(Markdown(global_summary))"
      ],
      "metadata": {
        "id": "Bd4Aw4-EbxjE"
      },
      "id": "Bd4Aw4-EbxjE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Análisis Multi-Cliente (Opcional)\n",
        "# Une todos los datos y genera análisis cruzado.\n",
        "\n",
        "# %% [code]\n",
        "df_all = pd.concat([d['tickets'] for d in client_data.values()])\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(pd.crosstab(df_all['cliente'], df_all['status']), annot=True, fmt='d')\n",
        "plt.title('Conteo de Tickets por Cliente y Estado')\n",
        "plt.savefig(os.path.join(PROCESSED_DIR, 'heatmap_client_status.png'))\n",
        "plt.show()\n",
        "\n",
        "md = f\"### Análisis Multi-Cliente\\n- Total de tickets: {len(df_all)}\\n- Cliente con más tickets: {df_all['cliente'].value_counts().idxmax()} con {df_all['cliente'].value_counts().max()} registros.\"\n",
        "display(Markdown(md))\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Documentación Dinámica Final\n",
        "# Resumen global de los análisis.\n",
        "\n",
        "# %% [code]\n",
        "global_summary = f\"\"\"\n",
        "# Resumen Final\n",
        "- **Total Clientes Procesados**: {len(client_data)}\n",
        "- **Total Tickets**: {len(all_tickets)}\n",
        "- **Porcentaje de Nulos**: {all_tickets.isnull().mean().to_dict()}\n",
        "- **Top Insight**: El cliente {all_tickets['cliente'].value_counts().idxmax()} tiene el mayor número de tickets ({all_tickets['cliente'].value_counts().max()}).\n",
        "\"\"\"\n",
        "display(Markdown(global_summary))\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 11. Checklist de Calidad\n",
        "# - ✅ Procesamiento uno por uno.\n",
        "# - ✅ Extracción de cliente de JSON/file.\n",
        "# - ✅ DataFrames por cliente.\n",
        "# - ✅ Supabase upsert/queries (si configurado).\n",
        "# - ✅ Gráficos + Markdown explicaciones.\n",
        "# - ✅ TZ Chile, limpieza.\n",
        "\"\"\"\n",
        "\n",
        "---\n",
        "\n",
        "### Instrucciones para Ejecutar en Colab:\n",
        "1. **Copia y pega el código** en un nuevo notebook de Google Colab.\n",
        "2. **Configura Supabase (opcional):**\n",
        "   - Cambia `USE_SUPABASE = True` si quieres usar Supabase.\n",
        "   - Reemplaza `SUPABASE_URL` y `SUPABASE_ANON_KEY` con tus credenciales de Supabase (puedes obtenerlas desde el dashboard de Supabase).\n",
        "3. **Carga tus archivos:**\n",
        "   - Ejecuta la celda 2. Se abrirá un botón para subir tus archivos JSON/Excel (e.g., JIRA_TIK10006.json, SERV-2055.json).\n",
        "4. **Selecciona archivo (opcional):**\n",
        "   - Si quieres procesar un archivo específico (e.g., 'SERV-2055.json'), modifica `SELECTED_FILE = 'SERV-2055.json'` en la celda 1 antes de ejecutar.\n",
        "5. **Ejecuta todas las celdas** secuencialmente. Observa los outputs (DataFrames, gráficos, Markdown) que se generan.\n",
        "6. **Revisa resultados:**\n",
        "   - Archivos procesados se guardan en `/content/data/processed/`.\n",
        "   - Gráficos y exportaciones (Excel/Parquet) se generan por cliente.\n",
        "\n",
        "### Notas:\n",
        "- Si no configuras Supabase, el notebook usará DuckDB local como fallback.\n",
        "- Asegúrate de tener suficiente espacio en Colab (máximo 68 GB, según tu captura).\n",
        "- Para consultas personalizadas, modifica las queries en la sección 6.\n",
        "\n",
        "¡Listo para analizar tus datos! Si necesitas ajustes, indícalos y te ayudaré a refinarlo."
      ],
      "metadata": {
        "id": "wTw6PwoSWMPd"
      },
      "id": "wTw6PwoSWMPd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# JIRA JSON → Excel (Tickets, Transitions, Comments, Worklogs)\n",
        "# Robusto para Colab | TZ Chile | JSON issue / search / list\n",
        "# ============================================================\n",
        "\n",
        "!pip -q install pandas openpyxl python-dateutil pytz\n",
        "\n",
        "import os, json, re, unicodedata, logging, glob\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from dateutil.parser import isoparse\n",
        "import pytz\n",
        "\n",
        "# ----------------- Configuración -----------------\n",
        "BASE_DIR = \"/content/jira\"\n",
        "RAW_DIR  = f\"{BASE_DIR}/raw\"        # Carpeta donde estarán los JSON (si no subes por upload)\n",
        "PROC_DIR = f\"{BASE_DIR}/processed\"  # Salidas Excel\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "os.makedirs(PROC_DIR, exist_ok=True)\n",
        "\n",
        "# \"upload\" -> te pide subir archivos; \"local\" -> lee desde RAW_DIR (glob)\n",
        "MODE = \"upload\"  # \"upload\" | \"local\"\n",
        "\n",
        "# Zona horaria Chile (manejo DST correcto)\n",
        "CL_TZ_NAME = \"America/Santiago\"\n",
        "_CL_TZ = pytz.timezone(CL_TZ_NAME)\n",
        "_UTC   = pytz.UTC\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "\n",
        "# ----------------- Utilidades -----------------\n",
        "_CONTROL_RE = re.compile(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]\")\n",
        "\n",
        "STATUS_MAP = {\n",
        "    \"Open\": \"ABIERTO\", \"To Do\": \"ABIERTO\",\n",
        "    \"In Progress\": \"EN_CURSO\", \"En Curso\": \"EN_CURSO\",\n",
        "    \"Blocked\": \"BLOQUEADO\",\n",
        "    \"Done\": \"CERRADO\", \"Closed\": \"CERRADO\", \"Resolved\": \"RESUELTO\",\n",
        "}\n",
        "\n",
        "def clean_text(x: Any) -> str:\n",
        "    if x is None: return \"\"\n",
        "    s = unicodedata.normalize(\"NFC\", str(x))\n",
        "    s = _CONTROL_RE.sub(\" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def norm_status(s: Optional[str]) -> str:\n",
        "    if not s: return \"\"\n",
        "    return STATUS_MAP.get(s, s.upper())\n",
        "\n",
        "def parse_ts_to_utc(ts: Optional[str]) -> Optional[datetime]:\n",
        "    \"\"\"Parsea ISO; si no trae TZ, asume Chile; devuelve UTC (aware).\"\"\"\n",
        "    if not ts: return None\n",
        "    try:\n",
        "        dt = isoparse(ts)\n",
        "        if dt.tzinfo is None:\n",
        "            dt = _CL_TZ.localize(dt)\n",
        "        return dt.astimezone(_UTC)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Fecha inválida {ts!r}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ----------------- Carga de archivos -----------------\n",
        "def collect_paths() -> List[str]:\n",
        "    if MODE == \"upload\":\n",
        "        from google.colab import files\n",
        "        up = files.upload()  # abre diálogo\n",
        "        saved = []\n",
        "        for name, content in up.items():\n",
        "            out = os.path.join(RAW_DIR, name)\n",
        "            with open(out, \"wb\") as f:\n",
        "                f.write(content)\n",
        "            saved.append(out)\n",
        "        return saved\n",
        "    elif MODE == \"local\":\n",
        "        return sorted(glob.glob(os.path.join(RAW_DIR, \"*.json\")) +\n",
        "                      glob.glob(os.path.join(RAW_DIR, \"*.jsonl\")))\n",
        "    else:\n",
        "        raise ValueError(\"MODE debe ser 'upload' o 'local'\")\n",
        "\n",
        "# ----------------- Flatten: issue -> 4 tablas -----------------\n",
        "def _flatten_issue_core(issue: Dict[str, Any], source_name: str\n",
        ") -> Tuple[Dict[str, Any], List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
        "    f = issue.get(\"fields\") or {}\n",
        "\n",
        "    ticket = {\n",
        "        \"source_file\": source_name,\n",
        "        \"issue_key\": issue.get(\"key\", \"\"),\n",
        "        \"issue_id\": issue.get(\"id\", \"\"),\n",
        "        \"project_key\": (f.get(\"project\") or {}).get(\"key\", \"\"),\n",
        "        \"summary\": clean_text(f.get(\"summary\")),\n",
        "        \"description\": clean_text(f.get(\"description\")),\n",
        "        \"status\": norm_status((f.get(\"status\") or {}).get(\"name\")),\n",
        "        \"priority\": (f.get(\"priority\") or {}).get(\"name\", \"\"),\n",
        "        \"issuetype\": (f.get(\"issuetype\") or {}).get(\"name\", \"\"),\n",
        "        \"assignee\": (f.get(\"assignee\") or {}).get(\"displayName\", \"\"),\n",
        "        \"created_utc\": parse_ts_to_utc(f.get(\"created\")),\n",
        "        \"updated_utc\": parse_ts_to_utc(f.get(\"updated\")),\n",
        "        \"resolutiondate_utc\": parse_ts_to_utc(f.get(\"resolutiondate\")),\n",
        "    }\n",
        "\n",
        "    transitions, comments, worklogs = [], [], []\n",
        "\n",
        "    # changelog -> transitions\n",
        "    for h in (issue.get(\"changelog\") or {}).get(\"histories\", []) or []:\n",
        "        ts = parse_ts_to_utc(h.get(\"created\"))\n",
        "        for it in h.get(\"items\", []) or []:\n",
        "            if it.get(\"field\") == \"status\":\n",
        "                transitions.append({\n",
        "                    \"source_file\": source_name,\n",
        "                    \"issue_key\": ticket[\"issue_key\"],\n",
        "                    \"changed_utc\": ts,\n",
        "                    \"from_status\": norm_status(it.get(\"fromString\")),\n",
        "                    \"to_status\": norm_status(it.get(\"toString\")),\n",
        "                    \"author\": (h.get(\"author\") or {}).get(\"displayName\", \"\")\n",
        "                })\n",
        "\n",
        "    # worklogs\n",
        "    wl = f.get(\"worklog\") or {}\n",
        "    if isinstance(wl, dict):\n",
        "        for w in wl.get(\"worklogs\", []) or []:\n",
        "            worklogs.append({\n",
        "                \"source_file\": source_name,\n",
        "                \"issue_key\": ticket[\"issue_key\"],\n",
        "                \"author\": (w.get(\"author\") or {}).get(\"displayName\", \"\"),\n",
        "                \"time_spent_sec\": w.get(\"timeSpentSeconds\") or 0,\n",
        "                \"started_utc\": parse_ts_to_utc(w.get(\"started\")),\n",
        "            })\n",
        "\n",
        "    # comments\n",
        "    cm = f.get(\"comment\") or {}\n",
        "    if isinstance(cm, dict):\n",
        "        for c in cm.get(\"comments\", []) or []:\n",
        "            comments.append({\n",
        "                \"source_file\": source_name,\n",
        "                \"issue_key\": ticket[\"issue_key\"],\n",
        "                \"author\": (c.get(\"author\") or {}).get(\"displayName\", \"\"),\n",
        "                \"created_utc\": parse_ts_to_utc(c.get(\"created\")),\n",
        "                \"body\": clean_text(c.get(\"body\")),\n",
        "            })\n",
        "\n",
        "    return ticket, transitions, comments, worklogs\n",
        "\n",
        "def flatten_any_jira_json(obj: Any, source_name: str\n",
        ") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
        "    \"\"\"Acepta: dict con 'fields', dict con 'issues', o lista de issues.\"\"\"\n",
        "    tickets, transitions, comments, worklogs = [], [], [], []\n",
        "    def _emit(issue: Dict[str, Any]):\n",
        "        t, tr, cm, wl = _flatten_issue_core(issue, source_name)\n",
        "        if t.get(\"issue_key\"):  # sólo si hay key\n",
        "            tickets.append(t); transitions += tr; comments += cm; worklogs += wl\n",
        "\n",
        "    if isinstance(obj, dict) and \"fields\" in obj:\n",
        "        _emit(obj)\n",
        "    elif isinstance(obj, dict) and \"issues\" in obj:\n",
        "        for issue in obj.get(\"issues\") or []:\n",
        "            if isinstance(issue, dict):\n",
        "                _emit(issue)\n",
        "    elif isinstance(obj, list):\n",
        "        for issue in obj:\n",
        "            if isinstance(issue, dict):\n",
        "                _emit(issue)\n",
        "    else:\n",
        "        logging.warning(f\"[{source_name}] Estructura no reconocida.\")\n",
        "\n",
        "    return tickets, transitions, comments, worklogs\n",
        "\n",
        "# ----------------- Pipeline JSON -> Excel -----------------\n",
        "def jsons_to_excel():\n",
        "    paths = collect_paths()\n",
        "    if not paths:\n",
        "        logging.warning(\"No se encontraron archivos.\")\n",
        "        return\n",
        "\n",
        "    # acumuladores para consolidado\n",
        "    all_tk, all_tr, all_cm, all_wl = [], [], [], []\n",
        "\n",
        "    for p in paths:\n",
        "        src = os.path.basename(p)\n",
        "        logging.info(f\"Procesando {src} ...\")\n",
        "\n",
        "        file_tk, file_tr, file_cm, file_wl = [], [], [], []\n",
        "\n",
        "        try:\n",
        "            if p.lower().endswith(\".jsonl\"):\n",
        "                with open(p, \"r\", encoding=\"utf-8\") as fh:\n",
        "                    for line in fh:\n",
        "                        if not line.strip():\n",
        "                            continue\n",
        "                        obj = json.loads(line)\n",
        "                        t, tr, cm, wl = flatten_any_jira_json(obj, src)\n",
        "                        file_tk += t; file_tr += tr; file_cm += cm; file_wl += wl\n",
        "            else:\n",
        "                with open(p, \"r\", encoding=\"utf-8\") as fh:\n",
        "                    obj = json.load(fh)\n",
        "                t, tr, cm, wl = flatten_any_jira_json(obj, src)\n",
        "                file_tk += t; file_tr += tr; file_cm += cm; file_wl += wl\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error leyendo {src}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # DataFrames del archivo actual\n",
        "        df_tk = pd.DataFrame(file_tk).drop_duplicates(subset=[\"issue_key\"]) if file_tk else pd.DataFrame()\n",
        "        df_tr = pd.DataFrame(file_tr).drop_duplicates() if file_tr else pd.DataFrame()\n",
        "        df_cm = pd.DataFrame(file_cm).drop_duplicates() if file_cm else pd.DataFrame()\n",
        "        df_wl = pd.DataFrame(file_wl).drop_duplicates() if file_wl else pd.DataFrame()\n",
        "\n",
        "        # Guardar Excel por archivo\n",
        "        out_xlsx = os.path.join(PROC_DIR, f\"{os.path.splitext(src)[0]}_jira.xlsx\")\n",
        "        with pd.ExcelWriter(out_xlsx, engine=\"openpyxl\") as xw:\n",
        "            if not df_tk.empty: df_tk.to_excel(xw, sheet_name=\"Tickets\", index=False)\n",
        "            if not df_tr.empty: df_tr.to_excel(xw, sheet_name=\"Transitions\", index=False)\n",
        "            if not df_cm.empty: df_cm.to_excel(xw, sheet_name=\"Comments\", index=False)\n",
        "            if not df_wl.empty: df_wl.to_excel(xw, sheet_name=\"Worklogs\", index=False)\n",
        "        logging.info(f\"Excel generado: {out_xlsx}\")\n",
        "\n",
        "        # acumular para consolidado\n",
        "        all_tk += file_tk; all_tr += file_tr; all_cm += file_cm; all_wl += file_wl\n",
        "\n",
        "    # ----------------- Excel Consolidado -----------------\n",
        "    cons_tk = pd.DataFrame(all_tk).drop_duplicates(subset=[\"issue_key\"]) if all_tk else pd.DataFrame()\n",
        "    cons_tr = pd.DataFrame(all_tr).drop_duplicates() if all_tr else pd.DataFrame()\n",
        "    cons_cm = pd.DataFrame(all_cm).drop_duplicates() if all_cm else pd.DataFrame()\n",
        "    cons_wl = pd.DataFrame(all_wl).drop_duplicates() if all_wl else pd.DataFrame()\n",
        "\n",
        "    out_all = os.path.join(PROC_DIR, \"JIRA_Consolidado.xlsx\")\n",
        "    with pd.ExcelWriter(out_all, engine=\"openpyxl\") as xw:\n",
        "        if not cons_tk.empty: cons_tk.to_excel(xw, sheet_name=\"Tickets\", index=False)\n",
        "        if not cons_tr.empty: cons_tr.to_excel(xw, sheet_name=\"Transitions\", index=False)\n",
        "        if not cons_cm.empty: cons_cm.to_excel(xw, sheet_name=\"Comments\", index=False)\n",
        "        if not cons_wl.empty: cons_wl.to_excel(xw, sheet_name=\"Worklogs\", index=False)\n",
        "    logging.info(f\"Excel consolidado generado: {out_all}\")\n",
        "\n",
        "# Ejecuta el pipeline\n",
        "jsons_to_excel()\n",
        "\n",
        "print(\"\\nListo. Revisa los .xlsx en:\", PROC_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Oc23qLyWfDxr",
        "outputId": "59925345-6792-4477-c330-a714bdc9807a"
      },
      "id": "Oc23qLyWfDxr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f0b5fabb-52cc-4cc4-8356-7228373c13ab\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f0b5fabb-52cc-4cc4-8356-7228373c13ab\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error leyendo HISOTIRCO_JIRA_2024-12-31 (2).json: cannot access local variable 'transitions' where it is not associated with a value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving HISOTIRCO_JIRA_2024-12-31.json to HISOTIRCO_JIRA_2024-12-31 (2).json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "At least one sheet must be visible",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3434318371.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;31m# Ejecuta el pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m \u001b[0mjsons_to_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nListo. Revisa los .xlsx en:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPROC_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3434318371.py\u001b[0m in \u001b[0;36mjsons_to_excel\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mout_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROC_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"JIRA_Consolidado.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"openpyxl\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcons_tk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcons_tk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Tickets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcons_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcons_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Transitions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTracebackType\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     ) -> None:\n\u001b[0;32m-> 1353\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0;34m\"\"\"synonym for save, to make it more file-like\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mSave\u001b[0m \u001b[0mworkbook\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"r+\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;31m# truncate file to the written content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openpyxl/workbook/workbook.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_only\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworksheets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_sheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0msave_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openpyxl/writer/excel.py\u001b[0m in \u001b[0;36msave_workbook\u001b[0;34m(workbook, filename)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0mworkbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimezone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtzinfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openpyxl/writer/excel.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;34m\"\"\"Write data into the archive.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_archive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openpyxl/writer/excel.py\u001b[0m in \u001b[0;36mwrite_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWorkbookWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkbook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARC_ROOT_RELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_root_rels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARC_WORKBOOK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARC_WORKBOOK_RELS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_rels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openpyxl/workbook/_writer.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_pivots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_views\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_refs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openpyxl/workbook/_writer.py\u001b[0m in \u001b[0;36mwrite_views\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_views\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mactive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_active_sheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactiveTab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openpyxl/workbook/_writer.py\u001b[0m in \u001b[0;36mget_active_sheet\u001b[0;34m(wb)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mvisible_sheets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sheets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msheet_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"visible\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvisible_sheets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one sheet must be visible\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_sheet_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: At least one sheet must be visible"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# JIRA → Excel Consolidado (Tickets, Transitions, Comments, Worklogs)\n",
        "# Robusto para Colab | TZ Chile | Soporta múltiples JSON/JSONL y descarga\n",
        "# ============================================================\n",
        "\n",
        "# !pip -q install pandas openpyxl requests pytz python-dateutil\n",
        "\n",
        "import os, json, re, unicodedata, glob, logging\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import pytz\n",
        "import requests\n",
        "from requests.auth import HTTPBasicAuth\n",
        "from dateutil.parser import isoparse\n",
        "\n",
        "# ----------------- Configuración general -----------------\n",
        "BASE_DIR = \"/content/jira\"\n",
        "RAW_DIR  = f\"{BASE_DIR}/raw\"        # Carpeta con JSON/JSONL locales\n",
        "PROC_DIR = f\"{BASE_DIR}/processed\"  # Salida Excel\n",
        "os.makedirs(RAW_DIR, exist_ok=True)\n",
        "os.makedirs(PROC_DIR, exist_ok=True)\n",
        "\n",
        "# Modo de carga: \"upload\" (subir) | \"local\" (leer de carpeta)\n",
        "MODE = \"upload\"\n",
        "\n",
        "# Lista de endpoints opcionales a descargar (se suman al consolidado)\n",
        "PROYECTOS = [\n",
        "    # Ejemplo:\n",
        "    # {\n",
        "    #   \"nombre\": \"SERV-2055\",\n",
        "    #   \"url\": \"https://vps.clickbi.cl/rapi_ges/v1/get_jira_issue_detail/SERV-2055\",\n",
        "    #   \"archivo\": \"jira_SERV-2055.json\"\n",
        "    # },\n",
        "]\n",
        "\n",
        "# Autenticación desde variables de entorno (opcional)\n",
        "# CLICKBI_TOKEN      -> Bearer <token>\n",
        "# EMAIL + JIRA_API_TOKEN -> Basic Auth\n",
        "MAX_RETRIES = 3\n",
        "TIMEOUT = 60\n",
        "\n",
        "# Zona horaria Chile\n",
        "CL_TZ_NAME = \"America/Santiago\"\n",
        "_CL_TZ = pytz.timezone(CL_TZ_NAME)\n",
        "_UTC   = pytz.UTC\n",
        "\n",
        "# Logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ----------------- Utilidades de texto y fechas -----------------\n",
        "_CONTROL_RE = re.compile(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]\")\n",
        "\n",
        "STATUS_MAP = {\n",
        "    \"Open\": \"ABIERTO\", \"To Do\": \"ABIERTO\",\n",
        "    \"In Progress\": \"EN_CURSO\", \"En Curso\": \"EN_CURSO\",\n",
        "    \"Blocked\": \"BLOQUEADO\",\n",
        "    \"Done\": \"CERRADO\", \"Closed\": \"CERRADO\", \"Resolved\": \"RESUELTO\",\n",
        "}\n",
        "\n",
        "def clean_text(x: Any) -> str:\n",
        "    if x is None: return \"\"\n",
        "    s = unicodedata.normalize(\"NFC\", str(x))\n",
        "    s = _CONTROL_RE.sub(\" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def norm_status(s: Optional[str]) -> str:\n",
        "    if not s: return \"\"\n",
        "    return STATUS_MAP.get(s, s.upper())\n",
        "\n",
        "def parse_ts_to_utc(ts: Optional[str]):\n",
        "    if not ts: return None\n",
        "    try:\n",
        "        dt = isoparse(ts)\n",
        "        if dt.tzinfo is None:\n",
        "            dt = _CL_TZ.localize(dt)\n",
        "        return dt.astimezone(_UTC)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Fecha inválida {ts!r}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ----------------- Autenticación -----------------\n",
        "def auth_headers() -> Dict[str, str]:\n",
        "    h = {\"User-Agent\": \"jira-etl/1.0\"}\n",
        "    tk = os.getenv(\"CLICKBI_TOKEN\")\n",
        "    if tk:\n",
        "        h[\"Authorization\"] = f\"Bearer {tk}\"\n",
        "    return h\n",
        "\n",
        "def basic_auth() -> Optional[HTTPBasicAuth]:\n",
        "    email = os.getenv(\"EMAIL\")\n",
        "    token = os.getenv(\"JIRA_API_TOKEN\")\n",
        "    if email and token:\n",
        "        return HTTPBasicAuth(email, token)\n",
        "    return None\n",
        "\n",
        "# ----------------- Descarga de JSON opcional -----------------\n",
        "def descargar_json(url: str, destino: str) -> bool:\n",
        "    h = auth_headers()\n",
        "    ba = basic_auth()\n",
        "\n",
        "    for i in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            r = requests.get(url, headers=h, auth=ba, timeout=TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "            with open(destino, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(r.json(), f, ensure_ascii=False, indent=2)\n",
        "            logger.info(f\"Descargado: {destino}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Intento {i}/{MAX_RETRIES} fallo descargando {url}: {e}\")\n",
        "    logger.error(f\"No se pudo descargar {url}\")\n",
        "    return False\n",
        "\n",
        "# ----------------- Carga de archivos -----------------\n",
        "def collect_paths() -> List[str]:\n",
        "    paths = []\n",
        "    # 1) Descargas configuradas\n",
        "    for p in PROYECTOS:\n",
        "        url = p.get(\"url\")\n",
        "        fn  = p.get(\"archivo\") or f\"{p.get('nombre','issue')}.json\"\n",
        "        out = os.path.join(RAW_DIR, fn)\n",
        "        if url:\n",
        "            descargar_json(url, out)\n",
        "        paths.append(out)\n",
        "\n",
        "    # 2) Local o Upload adicional\n",
        "    if MODE == \"upload\":\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            up = files.upload()\n",
        "            for name, content in up.items():\n",
        "                dest = os.path.join(RAW_DIR, name)\n",
        "                with open(dest, \"wb\") as f:\n",
        "                    f.write(content)\n",
        "                paths.append(dest)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Upload cancelado o fallido: {e}\")\n",
        "    elif MODE == \"local\":\n",
        "        paths += sorted(glob.glob(os.path.join(RAW_DIR, \"*.json\")) +\n",
        "                        glob.glob(os.path.join(RAW_DIR, \"*.jsonl\")))\n",
        "    # limpiar duplicados preservando orden\n",
        "    seen = set()\n",
        "    uniq = []\n",
        "    for p in paths:\n",
        "        if p not in seen:\n",
        "            uniq.append(p); seen.add(p)\n",
        "    return uniq\n",
        "\n",
        "# ----------------- Flatten: issue -> 4 tablas -----------------\n",
        "def _flatten_issue_core(issue: Dict[str, Any], source_name: str):\n",
        "    f = issue.get(\"fields\") or {}\n",
        "\n",
        "    ticket = {\n",
        "        \"source_file\": source_name,\n",
        "        \"issue_key\": issue.get(\"key\", \"\"),\n",
        "        \"issue_id\": issue.get(\"id\", \"\"),\n",
        "        \"project_key\": (f.get(\"project\") or {}).get(\"key\", \"\"),\n",
        "        \"summary\": clean_text(f.get(\"summary\")),\n",
        "        \"description\": clean_text(f.get(\"description\")),\n",
        "        \"status\": norm_status((f.get(\"status\") or {}).get(\"name\")),\n",
        "        \"priority\": (f.get(\"priority\") or {}).get(\"name\", \"\"),\n",
        "        \"issuetype\": (f.get(\"issuetype\") or {}).get(\"name\", \"\"),\n",
        "        \"assignee\": (f.get(\"assignee\") or {}).get(\"displayName\", \"\"),\n",
        "        \"created_utc\": parse_ts_to_utc(f.get(\"created\")),\n",
        "        \"updated_utc\": parse_ts_to_utc(f.get(\"updated\")),\n",
        "        \"resolutiondate_utc\": parse_ts_to_utc(f.get(\"resolutiondate\")),\n",
        "    }\n",
        "\n",
        "    transitions, comments, worklogs = [], [], []\n",
        "\n",
        "    for h in (issue.get(\"changelog\") or {}).get(\"histories\", []) or []:\n",
        "        ts = parse_ts_to_utc(h.get(\"created\"))\n",
        "        for it in h.get(\"items\", []) or []:\n",
        "            if it.get(\"field\") == \"status\":\n",
        "                transitions.append({\n",
        "                    \"source_file\": source_name,\n",
        "                    \"issue_key\": ticket[\"issue_key\"],\n",
        "                    \"changed_utc\": ts,\n",
        "                    \"from_status\": norm_status(it.get(\"fromString\")),\n",
        "                    \"to_status\": norm_status(it.get(\"toString\")),\n",
        "                    \"author\": (h.get(\"author\") or {}).get(\"displayName\", \"\"),\n",
        "                })\n",
        "\n",
        "    wl = f.get(\"worklog\") or {}\n",
        "    if isinstance(wl, dict):\n",
        "        for w in wl.get(\"worklogs\", []) or []:\n",
        "            worklogs.append({\n",
        "                \"source_file\": source_name,\n",
        "                \"issue_key\": ticket[\"issue_key\"],\n",
        "                \"author\": (w.get(\"author\") or {}).get(\"displayName\", \"\"),\n",
        "                \"time_spent_sec\": w.get(\"timeSpentSeconds\") or 0,\n",
        "                \"started_utc\": parse_ts_to_utc(w.get(\"started\")),\n",
        "            })\n",
        "\n",
        "    cm = f.get(\"comment\") or {}\n",
        "    if isinstance(cm, dict):\n",
        "        for c in cm.get(\"comments\", []) or []:\n",
        "            comments.append({\n",
        "                \"source_file\": source_name,\n",
        "                \"issue_key\": ticket[\"issue_key\"],\n",
        "                \"author\": (c.get(\"author\") or {}).get(\"displayName\", \"\"),\n",
        "                \"created_utc\": parse_ts_to_utc(c.get(\"created\")),\n",
        "                \"body\": clean_text(c.get(\"body\")),\n",
        "            })\n",
        "\n",
        "    return ticket, transitions, comments, worklogs\n",
        "\n",
        "def flatten_any_jira_json(obj: Any, source_name: str):\n",
        "    tickets, transitions, comments, worklogs = [], [], [], []\n",
        "\n",
        "    def _emit(issue: Dict[str, Any]):\n",
        "        t, tr, cm, wl = _flatten_issue_core(issue, source_name)\n",
        "        if t.get(\"issue_key\"):\n",
        "            tickets.append(t); transitions += tr; comments += cm; worklogs += wl\n",
        "\n",
        "    if isinstance(obj, dict) and \"fields\" in obj:\n",
        "        _emit(obj)\n",
        "    elif isinstance(obj, dict) and \"issues\" in obj:\n",
        "        for issue in obj.get(\"issues\") or []:\n",
        "            if isinstance(issue, dict): _emit(issue)\n",
        "    elif isinstance(obj, list):\n",
        "        for issue in obj:\n",
        "            if isinstance(issue, dict): _emit(issue)\n",
        "    else:\n",
        "        logger.warning(f\"[{source_name}] Estructura no reconocida.\")\n",
        "    return tickets, transitions, comments, worklogs\n",
        "\n",
        "# ----------------- Escritura Excel segura -----------------\n",
        "def _write_sheet(xw, df: pd.DataFrame, name: str):\n",
        "    \"\"\"Escribe una hoja si df tiene columnas; si no, no la crea.\"\"\"\n",
        "    if df is not None and not df.empty:\n",
        "        df.to_excel(xw, sheet_name=name, index=False)\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def write_excel_consolidado(dfs: Dict[str, pd.DataFrame], out_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Escribe Excel con múltiples hojas y garantiza al menos una visible.\n",
        "    Evita el error: 'At least one sheet must be visible'.\n",
        "    \"\"\"\n",
        "    with pd.ExcelWriter(out_path, engine=\"openpyxl\") as xw:\n",
        "        wrote_any = False\n",
        "        wrote_any |= _write_sheet(xw, dfs.get(\"tickets\"),     \"Tickets\")\n",
        "        wrote_any |= _write_sheet(xw, dfs.get(\"transitions\"), \"Transitions\")\n",
        "        wrote_any |= _write_sheet(xw, dfs.get(\"comments\"),    \"Comments\")\n",
        "        wrote_any |= _write_sheet(xw, dfs.get(\"worklogs\"),    \"Worklogs\")\n",
        "\n",
        "        if not wrote_any:\n",
        "            # Hoja de Resumen mínima para evitar IndexError\n",
        "            pd.DataFrame({\"info\": [\"No se encontraron datos válidos.\"]}).to_excel(\n",
        "                xw, sheet_name=\"Resumen\", index=False\n",
        "            )\n",
        "    return str(Path(out_path).resolve())\n",
        "\n",
        "# ----------------- Pipeline principal -----------------\n",
        "def run_pipeline() -> str:\n",
        "    paths = collect_paths()\n",
        "    if not paths:\n",
        "        logger.warning(\"No hay archivos para procesar.\")\n",
        "        out = os.path.join(PROC_DIR, \"JIRA_Consolidado.xlsx\")\n",
        "        return write_excel_consolidado({}, out)\n",
        "\n",
        "    all_tk, all_tr, all_cm, all_wl = [], [], [], []\n",
        "\n",
        "    for p in paths:\n",
        "        src = os.path.basename(p)\n",
        "        logger.info(f\"Procesando {src} ...\")\n",
        "        try:\n",
        "            if p.lower().endswith(\".jsonl\"):\n",
        "                with open(p, \"r\", encoding=\"utf-8\") as fh:\n",
        "                    for line in fh:\n",
        "                        if not line.strip(): continue\n",
        "                        obj = json.loads(line)\n",
        "                        t, tr, cm, wl = flatten_any_jira_json(obj, src)\n",
        "                        all_tk += t; all_tr += tr; all_cm += cm; all_wl += wl\n",
        "            else:\n",
        "                with open(p, \"r\", encoding=\"utf-8\") as fh:\n",
        "                    obj = json.load(fh)\n",
        "                t, tr, cm, wl = flatten_any_jira_json(obj, src)\n",
        "                all_tk += t; all_tr += tr; all_cm += wl  # <- ojo: esto estaba mal?\n",
        "                # Corrección: comments y worklogs\n",
        "                all_cm += cm\n",
        "                all_wl += wl\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error leyendo {src}: {e}\")\n",
        "\n",
        "    df_tk = pd.DataFrame(all_tk).drop_duplicates(subset=[\"issue_key\"]) if all_tk else pd.DataFrame()\n",
        "    df_tr = pd.DataFrame(all_tr).drop_duplicates() if all_tr else pd.DataFrame()\n",
        "    df_cm = pd.DataFrame(all_cm).drop_duplicates() if all_cm else pd.DataFrame()\n",
        "    df_wl = pd.DataFrame(all_wl).drop_duplicates() if all_wl else pd.DataFrame()\n",
        "\n",
        "    out_all = os.path.join(PROC_DIR, \"JIRA_Consolidado.xlsx\")\n",
        "    return write_excel_consolidado(\n",
        "        {\"tickets\": df_tk, \"transitions\": df_tr, \"comments\": df_cm, \"worklogs\": df_wl},\n",
        "        out_all\n",
        "    )\n",
        "\n",
        "# ----------------- Métricas (opcional) -----------------\n",
        "def enrich_with_cl_time(df: pd.DataFrame, col_utc: str, out_col: str) -> pd.DataFrame:\n",
        "    \"\"\"Agrega columna con hora local Chile (naive) desde UTC aware.\"\"\"\n",
        "    if col_utc in df.columns:\n",
        "        s = pd.to_datetime(df[col_utc], utc=True, errors=\"coerce\")\n",
        "        df[out_col] = s.dt.tz_convert(_CL_TZ).dt.tz_localize(None)\n",
        "    return df\n",
        "\n",
        "def post_metrics(out_xlsx_path: str):\n",
        "    \"\"\"Agrega una hoja 'Métricas' a un Excel ya creado (si existen datos).\"\"\"\n",
        "    try:\n",
        "        with pd.ExcelWriter(out_xlsx_path, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"overlay\") as xw:\n",
        "            # Cargar desde el archivo\n",
        "            xl = pd.ExcelFile(out_xlsx_path)\n",
        "            if \"Tickets\" in xl.sheet_names:\n",
        "                df_tk = pd.read_excel(out_xlsx_path, sheet_name=\"Tickets\")\n",
        "                # ejemplo de métrica\n",
        "                total = len(df_tk)\n",
        "                por_estado = df_tk[\"status\"].value_counts().rename_axis(\"status\").reset_index(name=\"count\") if \"status\" in df_tk.columns else pd.DataFrame()\n",
        "                resumen = pd.DataFrame({\"metrica\": [\"total_tickets\"], \"valor\": [total]})\n",
        "                resumen.to_excel(xw, sheet_name=\"Métricas\", index=False)\n",
        "                if not por_estado.empty:\n",
        "                    por_estado.to_excel(xw, sheet_name=\"Métricas\", index=False, startrow=3)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"No se pudieron escribir métricas: {e}\")\n",
        "\n",
        "# ----------------- Main -----------------\n",
        "def main():\n",
        "    out_path = run_pipeline()\n",
        "    # (Opcional) enriquecer hoja Tickets con horas CL si quieres reabrir y reescribir;\n",
        "    # o dejar esta función para cuando trabajes con DataFrames en memoria.\n",
        "    post_metrics(out_path)\n",
        "    print(\"\\n✅ Excel generado:\", out_path)\n",
        "    print(\"Carpeta:\", PROC_DIR)\n",
        "\n",
        "# Ejecutar\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "dSDmQWpffmMK",
        "outputId": "cc5e317f-1fa9-457d-8783-751bfda08a41"
      },
      "id": "dSDmQWpffmMK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-75fa6aa1-38e0-4808-90dd-b1f28a618b8c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-75fa6aa1-38e0-4808-90dd-b1f28a618b8c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error leyendo procesamiento_json (2).py: Expecting value: line 1 column 1 (char 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving procesamiento_json.py to procesamiento_json (2).py\n",
            "\n",
            "✅ Excel generado: /content/jira/processed/JIRA_Consolidado.xlsx\n",
            "Carpeta: /content/jira/processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/jira/raw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX8wn4tWg__f",
        "outputId": "e17323c0-2078-41d0-9044-17383fb38b14"
      },
      "id": "JX8wn4tWg__f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 32M\n",
            "-rw-r--r-- 1 root root 181K Oct 10 18:25 'CONSULTORTK_SERV-2055 (1) (1).json'\n",
            "-rw-r--r-- 1 root root  97K Oct 10 18:25 'FULLTICKET_SERV-516 (1).json'\n",
            "-rw-r--r-- 1 root root  97K Oct 10 19:06 'FULLTICKET_SERV-516 (2).json'\n",
            "-rw-r--r-- 1 root root 4.3M Oct 10 18:25 'gJu98ihCpUEIo-zR58OYr (1).png'\n",
            "-rw-r--r-- 1 root root  33K Oct 10 18:25 'HISOTIRCO_JIRA_2024-12-31 (1).json'\n",
            "-rw-r--r-- 1 root root  33K Oct 10 18:58 'HISOTIRCO_JIRA_2024-12-31 (2).json'\n",
            "-rw-r--r-- 1 root root  16M Oct 10 18:25 'jira_10013 (1).json'\n",
            "-rw-r--r-- 1 root root 5.4M Oct 10 18:25 'jira_10052 (1).json'\n",
            "-rw-r--r-- 1 root root  34K Oct 10 18:25 'jira_export (1).xlsx'\n",
            "-rw-r--r-- 1 root root  34K Oct 10 18:25 'jira_export_20251008_135834 (1).xlsx'\n",
            "-rw-r--r-- 1 root root 181K Oct 10 18:25 'JIRA_SERV-2055 (1) (1).json'\n",
            "-rw-r--r-- 1 root root  97K Oct 10 18:25 'jira_SERV-516 (1).json'\n",
            "-rw-r--r-- 1 root root 6.0M Oct 10 18:25 'JIRA_TIK10006 (1).json'\n",
            "-rw-r--r-- 1 root root  12K Oct 10 18:25 'procesamiento_json (1).py'\n",
            "-rw-r--r-- 1 root root  17K Oct 10 18:25 'procesar_jira (1).py'\n",
            "-rw-r--r-- 1 root root 254K Oct 10 18:25 'SERV-2055 (1).json'\n",
            "-rw-r--r-- 1 root root 182K Oct 10 18:25 'SERV-2055 (1).jsonl'\n",
            "-rw-r--r-- 1 root root 9.4K Oct 10 18:25 'SERV-2055_Export_08-10-2025 (1).xlsx'\n",
            "-rw-r--r-- 1 root root 3.2K Oct 10 18:25 'SERV-2055_export (1).xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Script para convertir archivos JSON de JIRA (issues, search, o JSONL) a Excel,\n",
        "separando Tickets, Transitions, Comments y Worklogs. Optimizado para Colab.\n",
        "\n",
        "Dependencias:\n",
        "    pip install pandas openpyxl python-dateutil pytz\n",
        "\n",
        "Uso:\n",
        "    - En Colab: Ejecuta directamente, sube archivos JSON via upload.\n",
        "    - Local: Configura MODE=\"local\" y coloca archivos en RAW_DIR.\n",
        "\n",
        "Variables de entorno: Ninguna requerida.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import unicodedata\n",
        "import logging\n",
        "import glob\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from dateutil.parser import isoparse\n",
        "import pytz\n",
        "from google.colab import files\n",
        "\n",
        "# ----------------- Configuración Global -----------------\n",
        "BASE_DIR = \"/content/jira\"\n",
        "RAW_DIR = os.path.join(BASE_DIR, \"raw\")\n",
        "PROC_DIR = os.path.join(BASE_DIR, \"processed\")\n",
        "\n",
        "# Crear directorios si no existen\n",
        "for directory in [RAW_DIR, PROC_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Modo de operación: \"upload\" para Colab, \"local\" para archivos locales\n",
        "MODE = \"upload\"  # \"upload\" | \"local\"\n",
        "\n",
        "# Zona horaria Chile (maneja DST automáticamente)\n",
        "CL_TZ = pytz.timezone(\"America/Santiago\")\n",
        "UTC = pytz.UTC\n",
        "\n",
        "# Configuración de logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ----------------- Constantes y Utilidades -----------------\n",
        "CONTROL_CHARS_RE = re.compile(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]\")\n",
        "\n",
        "STATUS_MAP = {\n",
        "    \"Open\": \"ABIERTO\", \"To Do\": \"ABIERTO\",\n",
        "    \"In Progress\": \"EN_CURSO\", \"En Curso\": \"EN_CURSO\",\n",
        "    \"Blocked\": \"BLOQUEADO\",\n",
        "    \"Done\": \"CERRADO\", \"Closed\": \"CERRADO\", \"Resolved\": \"RESUELTO\",\n",
        "}\n",
        "\n",
        "def clean_text(text: Any) -> str:\n",
        "    \"\"\"Limpia texto eliminando caracteres de control y normalizando.\"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    try:\n",
        "        text = unicodedata.normalize(\"NFC\", str(text))\n",
        "        text = CONTROL_CHARS_RE.sub(\" \", text)\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error limpiando texto: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def normalize_status(status: Optional[str]) -> str:\n",
        "    \"\"\"Normaliza el estado a mayúsculas o según STATUS_MAP.\"\"\"\n",
        "    if not status:\n",
        "        return \"\"\n",
        "    return STATUS_MAP.get(status, status.upper())\n",
        "\n",
        "def parse_timestamp_to_utc(ts: Optional[str]) -> Optional[datetime]:\n",
        "    \"\"\"Convierte un timestamp ISO a UTC, asumiendo Chile si no tiene TZ.\"\"\"\n",
        "    if not ts:\n",
        "        return None\n",
        "    try:\n",
        "        dt = isoparse(ts)\n",
        "        if dt.tzinfo is None:\n",
        "            dt = CL_TZ.localize(dt)\n",
        "        return dt.astimezone(UTC)\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Timestamp inválido {ts!r}: {e}\")\n",
        "        return None\n",
        "\n",
        "# ----------------- Gestión de Archivos -----------------\n",
        "def collect_file_paths() -> List[str]:\n",
        "    \"\"\"Recoge las rutas de los archivos JSON según el modo.\"\"\"\n",
        "    if MODE == \"upload\":\n",
        "        logger.info(\"Subiendo archivos JSON desde Colab...\")\n",
        "        uploaded = files.upload()\n",
        "        paths = []\n",
        "        for name, content in uploaded.items():\n",
        "            path = os.path.join(RAW_DIR, name)\n",
        "            with open(path, \"wb\") as f:\n",
        "                f.write(content)\n",
        "            paths.append(path)\n",
        "        logger.info(f\"Archivos subidos: {len(paths)}\")\n",
        "        return paths\n",
        "    elif MODE == \"local\":\n",
        "        paths = glob.glob(os.path.join(RAW_DIR, \"*.json\")) + glob.glob(os.path.join(RAW_DIR, \"*.jsonl\"))\n",
        "        logger.info(f\"Archivos encontrados localmente: {len(paths)}\")\n",
        "        return sorted(paths)\n",
        "    else:\n",
        "        raise ValueError(\"MODE debe ser 'upload' o 'local'\")\n",
        "\n",
        "# ----------------- Procesamiento de Datos -----------------\n",
        "def flatten_issue_core(issue: Dict[str, Any], source_name: str\n",
        ") -> Tuple[Dict[str, Any], List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
        "    \"\"\"Extrae datos básicos de un issue en 4 estructuras (ticket, transitions, comments, worklogs).\"\"\"\n",
        "    fields = issue.get(\"fields\", {})\n",
        "    ticket = {\n",
        "        \"source_file\": source_name,\n",
        "        \"issue_key\": issue.get(\"key\", \"\"),\n",
        "        \"issue_id\": issue.get(\"id\", \"\"),\n",
        "        \"project_key\": fields.get(\"project\", {}).get(\"key\", \"\"),\n",
        "        \"summary\": clean_text(fields.get(\"summary\")),\n",
        "        \"description\": clean_text(fields.get(\"description\")),\n",
        "        \"status\": normalize_status(fields.get(\"status\", {}).get(\"name\")),\n",
        "        \"priority\": fields.get(\"priority\", {}).get(\"name\", \"\"),\n",
        "        \"issuetype\": fields.get(\"issuetype\", {}).get(\"name\", \"\"),\n",
        "        \"assignee\": fields.get(\"assignee\", {}).get(\"displayName\", \"\"),\n",
        "        \"created_utc\": parse_timestamp_to_utc(fields.get(\"created\")),\n",
        "        \"updated_utc\": parse_timestamp_to_utc(fields.get(\"updated\")),\n",
        "        \"resolutiondate_utc\": parse_timestamp_to_utc(fields.get(\"resolutiondate\")),\n",
        "    }\n",
        "    transitions, comments, worklogs = [], [], []\n",
        "\n",
        "    # Transiciones\n",
        "    for hist in fields.get(\"changelog\", {}).get(\"histories\", []):\n",
        "        ts = parse_timestamp_to_utc(hist.get(\"created\"))\n",
        "        for item in hist.get(\"items\", []):\n",
        "            if item.get(\"field\") == \"status\":\n",
        "                transitions.append({\n",
        "                    \"source_file\": source_name,\n",
        "                    \"issue_key\": ticket[\"issue_key\"],\n",
        "                    \"changed_utc\": ts,\n",
        "                    \"from_status\": normalize_status(item.get(\"fromString\")),\n",
        "                    \"to_status\": normalize_status(item.get(\"toString\")),\n",
        "                    \"author\": hist.get(\"author\", {}).get(\"displayName\", \"\")\n",
        "                })\n",
        "\n",
        "    # Worklogs\n",
        "    for worklog in fields.get(\"worklog\", {}).get(\"worklogs\", []):\n",
        "        worklogs.append({\n",
        "            \"source_file\": source_name,\n",
        "            \"issue_key\": ticket[\"issue_key\"],\n",
        "            \"author\": worklog.get(\"author\", {}).get(\"displayName\", \"\"),\n",
        "            \"time_spent_sec\": worklog.get(\"timeSpentSeconds\", 0),\n",
        "            \"started_utc\": parse_timestamp_to_utc(worklog.get(\"started\")),\n",
        "        })\n",
        "\n",
        "    # Comentarios\n",
        "    for comment in fields.get(\"comment\", {}).get(\"comments\", []):\n",
        "        comments.append({\n",
        "            \"source_file\": source_name,\n",
        "            \"issue_key\": ticket[\"issue_key\"],\n",
        "            \"author\": comment.get(\"author\", {}).get(\"displayName\", \"\"),\n",
        "            \"created_utc\": parse_timestamp_to_utc(comment.get(\"created\")),\n",
        "            \"body\": clean_text(comment.get(\"body\")),\n",
        "        })\n",
        "\n",
        "    return ticket, transitions, comments, worklogs\n",
        "\n",
        "def flatten_jira_json(data: Any, source_name: str\n",
        ") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
        "    \"\"\"Procesa diferentes formatos de JSON (issue, search, lista) y los aplana.\"\"\"\n",
        "    tickets, transitions, comments, worklogs = [], [], [], []\n",
        "\n",
        "    def process_issue(issue: Dict[str, Any]):\n",
        "        if not isinstance(issue, dict) or \"fields\" not in issue:\n",
        "            logger.warning(f\"[{source_name}] Issue inválido: {issue}\")\n",
        "            return\n",
        "        ticket, tr, cm, wl = flatten_issue_core(issue, source_name)\n",
        "        if ticket.get(\"issue_key\"):\n",
        "            tickets.append(ticket)\n",
        "            transitions.extend(tr)\n",
        "            comments.extend(cm)\n",
        "            worklogs.extend(wl)\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        if \"fields\" in data:\n",
        "            process_issue(data)\n",
        "        elif \"issues\" in data:\n",
        "            for issue in data.get(\"issues\", []):\n",
        "                process_issue(issue)\n",
        "    elif isinstance(data, list):\n",
        "        for issue in data:\n",
        "            process_issue(issue)\n",
        "    else:\n",
        "        logger.warning(f\"[{source_name}] Formato JSON no soportado: {type(data)}\")\n",
        "\n",
        "    return tickets, transitions, comments, worklogs\n",
        "\n",
        "# ----------------- Generación de Excel -----------------\n",
        "def process_json_to_excel():\n",
        "    \"\"\"Pipeline completo: carga JSON, procesa y genera Excel por archivo y consolidado.\"\"\"\n",
        "    file_paths = collect_file_paths()\n",
        "    if not file_paths:\n",
        "        logger.error(\"No se encontraron archivos para procesar.\")\n",
        "        return\n",
        "\n",
        "    all_tickets, all_transitions, all_comments, all_worklogs = [], [], [], []\n",
        "\n",
        "    for path in file_paths:\n",
        "        source_name = os.path.basename(path)\n",
        "        logger.info(f\"Procesando {source_name} ...\")\n",
        "\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                if path.lower().endswith(\".jsonl\"):\n",
        "                    data = [json.loads(line) for line in f if line.strip()]\n",
        "                else:\n",
        "                    data = json.load(f)\n",
        "\n",
        "            tickets, transitions, comments, worklogs = flatten_jira_json(data, source_name)\n",
        "\n",
        "            # Validar y crear DataFrames\n",
        "            df_tickets = pd.DataFrame(tickets).drop_duplicates(subset=[\"issue_key\"]) if tickets else pd.DataFrame()\n",
        "            df_transitions = pd.DataFrame(transitions).drop_duplicates() if transitions else pd.DataFrame()\n",
        "            df_comments = pd.DataFrame(comments).drop_duplicates() if comments else pd.DataFrame()\n",
        "            df_worklogs = pd.DataFrame(worklogs).drop_duplicates() if worklogs else pd.DataFrame()\n",
        "\n",
        "            if df_tickets.empty and df_transitions.empty and df_comments.empty and df_worklogs.empty:\n",
        "                logger.warning(f\"[{source_name}] No se generaron datos válidos.\")\n",
        "                continue\n",
        "\n",
        "            # Exportar por archivo\n",
        "            output_file = os.path.join(PROC_DIR, f\"{os.path.splitext(source_name)[0]}_jira.xlsx\")\n",
        "            with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
        "                if not df_tickets.empty: df_tickets.to_excel(writer, sheet_name=\"Tickets\", index=False)\n",
        "                if not df_transitions.empty: df_transitions.to_excel(writer, sheet_name=\"Transitions\", index=False)\n",
        "                if not df_comments.empty: df_comments.to_excel(writer, sheet_name=\"Comments\", index=False)\n",
        "                if not df_worklogs.empty: df_worklogs.to_excel(writer, sheet_name=\"Worklogs\", index=False)\n",
        "            logger.info(f\"Exportado: {output_file}\")\n",
        "\n",
        "            # Acumular para consolidado\n",
        "            all_tickets.extend(tickets)\n",
        "            all_transitions.extend(transitions)\n",
        "            all_comments.extend(comments)\n",
        "            all_worklogs.extend(worklogs)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error procesando {source_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Exportar consolidado\n",
        "    if all_tickets or all_transitions or all_comments or all_worklogs:\n",
        "        cons_tickets = pd.DataFrame(all_tickets).drop_duplicates(subset=[\"issue_key\"]) if all_tickets else pd.DataFrame()\n",
        "        cons_transitions = pd.DataFrame(all_transitions).drop_duplicates() if all_transitions else pd.DataFrame()\n",
        "        cons_comments = pd.DataFrame(all_comments).drop_duplicates() if all_comments else pd.DataFrame()\n",
        "        cons_worklogs = pd.DataFrame(all_worklogs).drop_duplicates() if all_worklogs else pd.DataFrame()\n",
        "\n",
        "        output_cons = os.path.join(PROC_DIR, \"JIRA_Consolidado.xlsx\")\n",
        "        with pd.ExcelWriter(output_cons, engine=\"openpyxl\") as writer:\n",
        "            if not cons_tickets.empty: cons_tickets.to_excel(writer, sheet_name=\"Tickets\", index=False)\n",
        "            if not cons_transitions.empty: cons_transitions.to_excel(writer, sheet_name=\"Transitions\", index=False)\n",
        "            if not cons_comments.empty: cons_comments.to_excel(writer, sheet_name=\"Comments\", index=False)\n",
        "            if not cons_worklogs.empty: cons_worklogs.to_excel(writer, sheet_name=\"Worklogs\", index=False)\n",
        "        logger.info(f\"Exportado consolidado: {output_cons}\")\n",
        "    else:\n",
        "        logger.warning(\"No hay datos para el archivo consolidado.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"Iniciando procesamiento de JIRA JSON a Excel...\")\n",
        "    process_json_to_excel()\n",
        "    logger.info(f\"Proceso completado. Archivos en: {PROC_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "Mj43sOIBiDm3",
        "outputId": "d99a1577-7196-4481-a11f-8661a22a7456"
      },
      "id": "Mj43sOIBiDm3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b1423f77-52f9-452e-959e-3d82d347b583\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b1423f77-52f9-452e-959e-3d82d347b583\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error procesando SERV-2055 (2).json: Excel does not support datetimes with timezones. Please ensure that datetimes are timezone unaware before writing to Excel.\n",
            "WARNING:__main__:No hay datos para el archivo consolidado.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving SERV-2055.json to SERV-2055 (2).json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, os, itertools\n",
        "\n",
        "RAW = Path(\"/content/jira/raw\")\n",
        "\n",
        "def sniff_json(path: Path, max_preview=400):\n",
        "    txt = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
        "    head = txt[:max_preview].replace(\"\\n\",\" \")[:max_preview]\n",
        "    kind = None\n",
        "    keys = None\n",
        "    try:\n",
        "        obj = json.loads(txt)\n",
        "        kind = type(obj).__name__\n",
        "        if isinstance(obj, dict):\n",
        "            keys = list(itertools.islice(obj.keys(), 12))\n",
        "        elif isinstance(obj, list) and obj and isinstance(obj[0], dict):\n",
        "            keys = list(itertools.islice(obj[0].keys(), 12))\n",
        "    except Exception:\n",
        "        # ¿podría ser JSONL?\n",
        "        first_line = txt.splitlines()[0] if txt.splitlines() else \"\"\n",
        "        try:\n",
        "            obj = json.loads(first_line)\n",
        "            kind = \"jsonl\"\n",
        "            if isinstance(obj, dict):\n",
        "                keys = list(itertools.islice(obj.keys(), 12))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return head, kind, keys\n",
        "\n",
        "for f in sorted(RAW.glob(\"*\")):\n",
        "    if f.suffix.lower() not in [\".json\", \".jsonl\"]:\n",
        "        continue\n",
        "    head, kind, keys = sniff_json(f)\n",
        "    print(f\"\\n► {f.name}\")\n",
        "    print(\"   tipo:\", kind, \"| claves:\", keys if keys else \"(sin dict)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmVlBr96iS9Q",
        "outputId": "92b6ddfc-4261-4557-8031-4858f5f0795c"
      },
      "id": "XmVlBr96iS9Q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "► CONSULTORTK_SERV-2055 (1) (1).json\n",
            "   tipo: dict | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► FULLTICKET_SERV-516 (1).json\n",
            "   tipo: dict | claves: ['expand', 'id', 'self', 'key', 'renderedFields', 'names', 'schema', 'transitions', 'operations', 'editmeta', 'changelog', 'versionedRepresentations']\n",
            "\n",
            "► FULLTICKET_SERV-516 (2).json\n",
            "   tipo: dict | claves: ['expand', 'id', 'self', 'key', 'renderedFields', 'names', 'schema', 'transitions', 'operations', 'editmeta', 'changelog', 'versionedRepresentations']\n",
            "\n",
            "► FULLTICKET_SERV-516 (3).json\n",
            "   tipo: dict | claves: ['expand', 'id', 'self', 'key', 'renderedFields', 'names', 'schema', 'transitions', 'operations', 'editmeta', 'changelog', 'versionedRepresentations']\n",
            "\n",
            "► HISOTIRCO_JIRA_2024-12-31 (1).json\n",
            "   tipo: list | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► HISOTIRCO_JIRA_2024-12-31 (2).json\n",
            "   tipo: list | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► JIRA_SERV-2055 (1) (1).json\n",
            "   tipo: dict | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► JIRA_SERV-2055 (1) (2).json\n",
            "   tipo: dict | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► JIRA_TIK10006 (1).json\n",
            "   tipo: list | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► SERV-2055 (1).json\n",
            "   tipo: dict | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► SERV-2055 (1).jsonl\n",
            "   tipo: dict | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► SERV-2055 (2).json\n",
            "   tipo: dict | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► jira_10013 (1).json\n",
            "   tipo: list | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► jira_10013 (2).json\n",
            "   tipo: list | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► jira_10052 (1).json\n",
            "   tipo: list | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► jira_SERV-2055 (1).json\n",
            "   tipo: dict | claves: ['expand', 'id', 'self', 'key', 'fields']\n",
            "\n",
            "► jira_SERV-516 (1).json\n",
            "   tipo: dict | claves: ['expand', 'id', 'self', 'key', 'renderedFields', 'names', 'schema', 'transitions', 'operations', 'editmeta', 'changelog', 'versionedRepresentations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import openpyxl\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# ----------------- Configuración Global -----------------\n",
        "BASE_DIR = \"/content/jira\"\n",
        "RAW_DIR = os.path.join(BASE_DIR, \"raw\")\n",
        "PROC_DIR = os.path.join(BASE_DIR, \"processed\")\n",
        "\n",
        "# Crear directorios si no existen\n",
        "for directory in [RAW_DIR, PROC_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Configuración de logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ----------------- Utilidades -----------------\n",
        "def safe_access(d: Any, *keys, default: Any = \"\") -> Any:\n",
        "    \"\"\"Acceso seguro a claves anidadas en un diccionario.\"\"\"\n",
        "    current = d\n",
        "    for key in keys:\n",
        "        if not isinstance(current, dict) or current.get(key) is None:\n",
        "            return default\n",
        "        current = current.get(key)\n",
        "    return current\n",
        "\n",
        "def normalize_issue(issue: Dict[str, Any], source_name: str) -> Dict[str, Any]:\n",
        "    \"\"\"Normaliza un issue de JIRA a un diccionario plano con valores tolerantes a None.\"\"\"\n",
        "    fields = issue.get(\"fields\", {})\n",
        "    return {\n",
        "        \"file\": source_name,\n",
        "        \"issue_key\": issue.get(\"key\", \"\"),\n",
        "        \"issue_id\": issue.get(\"id\", \"\"),\n",
        "        \"project_key\": safe_access(fields, \"project\", \"key\"),\n",
        "        \"summary\": safe_access(fields, \"summary\"),\n",
        "        \"status\": safe_access(fields, \"status\", \"name\"),\n",
        "        \"priority\": safe_access(fields, \"priority\", \"name\"),\n",
        "        \"issuetype\": safe_access(fields, \"issuetype\", \"name\"),\n",
        "        \"assignee\": safe_access(fields, \"assignee\", \"displayName\"),\n",
        "        \"created\": safe_access(fields, \"created\"),\n",
        "        \"updated\": safe_access(fields, \"updated\"),\n",
        "        \"resolutiondate\": safe_access(fields, \"resolutiondate\"),\n",
        "        \"description\": safe_access(fields, \"description\"),\n",
        "    }\n",
        "\n",
        "def parse_issue_file(file_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Parsea un archivo JSON o JSONL y devuelve una lista de issues normalizados.\"\"\"\n",
        "    source_name = os.path.basename(file_path)\n",
        "    records = []\n",
        "\n",
        "    try:\n",
        "        if file_path.lower().endswith(\".jsonl\"):\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        continue\n",
        "                    try:\n",
        "                        data = json.loads(line)\n",
        "                        if isinstance(data, list):\n",
        "                            for item in data:\n",
        "                                if isinstance(item, dict) and (\"fields\" in item or \"key\" in item):\n",
        "                                    records.append(normalize_issue(item, source_name))\n",
        "                        elif isinstance(data, dict) and (\"fields\" in data or \"key\" in data):\n",
        "                            records.append(normalize_issue(data, source_name))\n",
        "                    except json.JSONDecodeError:\n",
        "                        logger.warning(f\"[{source_name}] Línea inválida ignorada.\")\n",
        "                        continue\n",
        "        else:  # .json\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "                if isinstance(data, dict) and (\"fields\" in data or \"key\" in data):\n",
        "                    records.append(normalize_issue(data, source_name))\n",
        "                elif isinstance(data, dict):\n",
        "                    for key in (\"issues\", \"tickets\", \"data\", \"results\", \"values\"):\n",
        "                        issues = data.get(key)\n",
        "                        if isinstance(issues, list):\n",
        "                            for item in issues:\n",
        "                                if isinstance(item, dict) and (\"fields\" in item or \"key\" in item):\n",
        "                                    records.append(normalize_issue(item, source_name))\n",
        "                            break\n",
        "                elif isinstance(data, list):\n",
        "                    for item in data:\n",
        "                        if isinstance(item, dict) and (\"fields\" in item or \"key\" in item):\n",
        "                            records.append(normalize_issue(item, source_name))\n",
        "\n",
        "        logger.info(f\"[{source_name}] Procesados {len(records)} issues.\")\n",
        "        return records\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"[{source_name}] Archivo no encontrado.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"[{source_name}] Error al procesar: {e}\")\n",
        "        return []\n",
        "\n",
        "# ----------------- Generación de Excel -----------------\n",
        "def generate_excel(dataframe: pd.DataFrame, output_path: str) -> None:\n",
        "    \"\"\"Genera un archivo Excel con hojas de Detalle y Resumen.\"\"\"\n",
        "    if dataframe.empty:\n",
        "        logger.warning(\"DataFrame vacío, creando hoja informativa.\")\n",
        "        df_info = pd.DataFrame({\"info\": [\"No se encontraron datos válidos.\"]})\n",
        "        with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
        "            df_info.to_excel(writer, sheet_name=\"info\", index=False)\n",
        "        return\n",
        "\n",
        "    # Convertir fechas a datetime, ignorando errores\n",
        "    date_cols = [\"created\", \"updated\", \"resolutiondate\"]\n",
        "    for col in date_cols:\n",
        "        if col in dataframe.columns:\n",
        "            dataframe[col] = pd.to_datetime(dataframe[col], errors=\"coerce\")\n",
        "\n",
        "    # Ordenar por fechas y claves\n",
        "    dataframe.sort_values([\"created\", \"issue_key\"], inplace=True, na_position=\"last\")\n",
        "\n",
        "    # Generar resumen\n",
        "    summary_sheets = []\n",
        "    if \"status\" in dataframe.columns:\n",
        "        status_counts = (dataframe.groupby(\"status\", dropna=False)[\"issue_key\"]\n",
        "                        .count().rename(\"cantidad\").reset_index())\n",
        "        summary_sheets.append((\"Tickets por estado\", status_counts))\n",
        "    if \"assignee\" in dataframe.columns:\n",
        "        top_assignees = (dataframe.groupby(\"assignee\", dropna=False)[\"issue_key\"]\n",
        "                        .count().rename(\"tickets\").sort_values(ascending=False)\n",
        "                        .head(10).reset_index())\n",
        "        summary_sheets.append((\"Top 10 asignados\", top_assignees))\n",
        "    if \"file\" in dataframe.columns:\n",
        "        file_counts = (dataframe.groupby(\"file\", dropna=False)[\"issue_key\"]\n",
        "                      .count().rename(\"tickets\").reset_index())\n",
        "        summary_sheets.append((\"Tickets por archivo\", file_counts))\n",
        "\n",
        "    # Escribir a Excel\n",
        "    try:\n",
        "        with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
        "            dataframe.to_excel(writer, sheet_name=\"Detalle\", index=False)\n",
        "            if summary_sheets:\n",
        "                workbook = writer.book\n",
        "                summary_sheet = workbook.create_sheet(\"Resumen\")\n",
        "                row = 1\n",
        "                for title, table in summary_sheets:\n",
        "                    summary_sheet.cell(row=row, column=1, value=title)\n",
        "                    row += 1\n",
        "                    for col, col_name in enumerate(table.columns, start=1):\n",
        "                        summary_sheet.cell(row=row, column=col, value=col_name)\n",
        "                    row += 1\n",
        "                    for _, row_data in table.iterrows():\n",
        "                        for col, value in enumerate(row_data.tolist(), start=1):\n",
        "                            summary_sheet.cell(row=row, column=col, value=None if pd.isna(value) else value)\n",
        "                        row += 1\n",
        "                    row += 2  # Espacio entre bloques\n",
        "        logger.info(f\"✅ Excel generado: {output_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al generar Excel: {e}\")\n",
        "        raise\n",
        "\n",
        "# ----------------- Pipeline Principal -----------------\n",
        "def main():\n",
        "    \"\"\"Ejecuta el pipeline completo de procesamiento de JIRA JSON a Excel.\"\"\"\n",
        "    logger.info(\"Iniciando procesamiento de archivos JIRA...\")\n",
        "    all_records = []\n",
        "\n",
        "    # Procesar cada archivo en RAW_DIR\n",
        "    for filename in os.listdir(RAW_DIR):\n",
        "        if not filename.lower().endswith((\".json\", \".jsonl\")):\n",
        "            continue\n",
        "        file_path = os.path.join(RAW_DIR, filename)\n",
        "        try:\n",
        "            records = parse_issue_file(file_path)\n",
        "            all_records.extend(records)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"⚠️ Error procesando {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Crear DataFrame\n",
        "    df = pd.DataFrame(all_records)\n",
        "    logger.info(f\"📦 Registros procesados: {len(df)}\")\n",
        "\n",
        "    # Generar archivo Excel\n",
        "    output_path = os.path.join(PROC_DIR, \"JIRA_Consolidado.xlsx\")\n",
        "    generate_excel(df, output_path)\n",
        "\n",
        "    # Mostrar vista previa\n",
        "    if not df.empty:\n",
        "        from IPython.display import display\n",
        "        display(df.head(10))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "ogXDwsWAnDU3",
        "outputId": "1c4dd2b6-5073-495b-cd60-6735437781b9"
      },
      "id": "ogXDwsWAnDU3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3861084489.py:116: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
            "  dataframe[col] = pd.to_datetime(dataframe[col], errors=\"coerce\")\n",
            "/tmp/ipython-input-3861084489.py:116: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
            "  dataframe[col] = pd.to_datetime(dataframe[col], errors=\"coerce\")\n",
            "/tmp/ipython-input-3861084489.py:116: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
            "  dataframe[col] = pd.to_datetime(dataframe[col], errors=\"coerce\")\n",
            "ERROR:__main__:Error al generar Excel: Excel does not support datetimes with timezones. Please ensure that datetimes are timezone unaware before writing to Excel.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Excel does not support datetimes with timezones. Please ensure that datetimes are timezone unaware before writing to Excel.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3861084489.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3861084489.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;31m# Generar archivo Excel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROC_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"JIRA_Consolidado.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mgenerate_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Mostrar vista previa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3861084489.py\u001b[0m in \u001b[0;36mgenerate_excel\u001b[0;34m(dataframe, output_path)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"openpyxl\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Detalle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msummary_sheets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mworkbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[0;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   2415\u001b[0m             \u001b[0minf_rep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minf_rep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m         )\n\u001b[0;32m-> 2417\u001b[0;31m         formatter.write(\n\u001b[0m\u001b[1;32m   2418\u001b[0m             \u001b[0mexcel_writer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m             \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/excel.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             writer._write_cells(\n\u001b[0m\u001b[1;32m    953\u001b[0m                 \u001b[0mformatted_cells\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                 \u001b[0msheet_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_openpyxl.py\u001b[0m in \u001b[0;36m_write_cells\u001b[0;34m(self, cells, sheet_name, startrow, startcol, freeze_panes)\u001b[0m\n\u001b[1;32m    484\u001b[0m             )\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcells\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m             xcell = wks.cell(\n\u001b[1;32m    488\u001b[0m                 \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartrow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstartcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/excel.py\u001b[0m in \u001b[0;36mget_formatted_cells\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    888\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_formatted_cells\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mExcelCell\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m             \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/formats/excel.py\u001b[0m in \u001b[0;36m_format_value\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_format\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tzinfo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    608\u001b[0m                 \u001b[0;34m\"Excel does not support datetimes with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;34m\"timezones. Please ensure that datetimes \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Excel does not support datetimes with timezones. Please ensure that datetimes are timezone unaware before writing to Excel."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cbBn6pmlnD9G"
      },
      "id": "cbBn6pmlnD9G",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}